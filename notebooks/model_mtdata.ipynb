{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Mission : Error Analysis & Model metadata``\n",
    "\n",
    "- Is this the problem with past submissions too??\n",
    "- Develop Better guess\n",
    "- Start from avgs, expanding means.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('../data/metadata.csv')\n",
    "sub_format = pd.read_csv('../data/submission_format.csv')\n",
    "train_labels = pd.read_csv('../data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mse(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dens_to_sev(x: float)-> int:\n",
    "    \"\"\"takes in density value in cells/ml and returns severity category\"\"\"\n",
    "    if (x < 20_000) : return 1\n",
    "    elif (x >= 20_000) and (x < 100_000) : return 2\n",
    "    elif (x >= 100_000) and (x < 1_000_000) : return 3\n",
    "    elif (x >= 1_000_000) and (x < 10_000_000) : return 4\n",
    "    elif x > 10_000_000 : return 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add date fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabm</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>train</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>585.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aacd</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>train</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaee</td>\n",
       "      <td>35.487000</td>\n",
       "      <td>-79.062133</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>train</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaff</td>\n",
       "      <td>38.049471</td>\n",
       "      <td>-99.827001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>train</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>3.0</td>\n",
       "      <td>111825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23566</th>\n",
       "      <td>zzwo</td>\n",
       "      <td>39.792190</td>\n",
       "      <td>-99.971050</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>train</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23567</th>\n",
       "      <td>zzwq</td>\n",
       "      <td>35.794000</td>\n",
       "      <td>-79.012551</td>\n",
       "      <td>2015-03-24</td>\n",
       "      <td>train</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23568</th>\n",
       "      <td>zzyb</td>\n",
       "      <td>35.742000</td>\n",
       "      <td>-79.238600</td>\n",
       "      <td>2016-11-21</td>\n",
       "      <td>train</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23570 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date  split  year  month  week  \\\n",
       "0      aabm  39.080319  -86.430867 2018-05-14  train  2018      5    20   \n",
       "1      aabn  36.559700 -121.510000 2016-08-31   test  2016      8    35   \n",
       "2      aacd  35.875083  -78.878434 2020-11-19  train  2020     11    47   \n",
       "3      aaee  35.487000  -79.062133 2016-08-24  train  2016      8    34   \n",
       "4      aaff  38.049471  -99.827001 2019-07-23  train  2019      7    30   \n",
       "...     ...        ...         ...        ...    ...   ...    ...   ...   \n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02   test  2014     12    49   \n",
       "23566  zzwo  39.792190  -99.971050 2017-06-19  train  2017      6    25   \n",
       "23567  zzwq  35.794000  -79.012551 2015-03-24  train  2015      3    13   \n",
       "23568  zzyb  35.742000  -79.238600 2016-11-21  train  2016     11    47   \n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31   test  2015      8    36   \n",
       "\n",
       "       season   region  severity   density  \n",
       "0           2  midwest       1.0     585.0  \n",
       "1           3     west       NaN       NaN  \n",
       "2           4    south       1.0     290.0  \n",
       "3           3    south       1.0    1614.0  \n",
       "4           3  midwest       3.0  111825.0  \n",
       "...       ...      ...       ...       ...  \n",
       "23565       1     west       NaN       NaN  \n",
       "23566       3  midwest       2.0   48510.0  \n",
       "23567       2    south       1.0    1271.0  \n",
       "23568       4    south       1.0    9682.0  \n",
       "23569       3  midwest       NaN       NaN  \n",
       "\n",
       "[23570 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.date = pd.to_datetime(metadata.date)\n",
    "metadata['year'] = metadata.date.dt.year\n",
    "metadata['month'] = metadata.date.dt.month\n",
    "metadata['week'] = metadata.date.dt.isocalendar().week\n",
    "\n",
    "\n",
    "seasons = {\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "    6: 3,\n",
    "    7: 3,\n",
    "    8: 3,\n",
    "    9: 4,\n",
    "    10: 4,\n",
    "    11: 4,\n",
    "    12: 1\n",
    "}\n",
    "\n",
    "metadata['season'] = metadata.month.map(seasons)\n",
    "\n",
    "\n",
    "region = pd.concat((train_labels, sub_format[['region', 'uid']]), axis=0)\n",
    "\n",
    "data = pd.merge(metadata, region, on='uid', how='left')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6510, 12), (23570, 12))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = data[data.split == 'test']\n",
    "test_data.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17060, 12), (23570, 12))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data[data.split == 'train']\n",
    "train_data.shape, data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Utils\n",
    "def get_data_by_date( date=None, data=train_data):\n",
    "    return data[data.date == date]\n",
    "\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "\n",
    "def analyize_matches(y_true, y_pred):\n",
    "    print(\"Exact matches: \", sum(y_true == y_pred) / len(y_true))\n",
    "    \n",
    "    print(\"Missed by 1: \", sum(abs(y_true - y_pred) == 1) / len(y_true))\n",
    "    print(\"Missed by 2: \", sum(abs(y_true - y_pred) == 2) / len(y_true))\n",
    "    print(\"Missed by 3: \", sum(abs(y_true - y_pred) == 3) / len(y_true))\n",
    "    print(\"Missed by 4: \", sum(abs(y_true - y_pred) == 4) / len(y_true))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Guess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Get a sample and try to guess it's severity based on location, region, date, season, month.\n",
    "\n",
    "> Trail 1:\n",
    "\n",
    "    1. Get a test/val sample\n",
    "\n",
    "    2. Get subset of train data from the sample's date and region.\n",
    "        2.1 if subset is empty, get subset of train data from the sample's previous date and region.\n",
    "        2.2. if stil the subset is empty, fill the severity with 2.\n",
    "    \n",
    "    3. Get the nearest sample from the subset and use it's severity as the guess.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6865</th>\n",
       "      <td>howw</td>\n",
       "      <td>37.0062</td>\n",
       "      <td>-120.600</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>eamn</td>\n",
       "      <td>36.9818</td>\n",
       "      <td>-120.221</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7668</th>\n",
       "      <td>imsv</td>\n",
       "      <td>36.9836</td>\n",
       "      <td>-120.500</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>wgxq</td>\n",
       "      <td>33.8011</td>\n",
       "      <td>-117.205</td>\n",
       "      <td>2013-01-25</td>\n",
       "      <td>test</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16095</th>\n",
       "      <td>rsos</td>\n",
       "      <td>33.8892</td>\n",
       "      <td>-117.562</td>\n",
       "      <td>2013-01-25</td>\n",
       "      <td>test</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12443</th>\n",
       "      <td>nsoi</td>\n",
       "      <td>36.7368</td>\n",
       "      <td>-121.734</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>test</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14254</th>\n",
       "      <td>prfi</td>\n",
       "      <td>36.7518</td>\n",
       "      <td>-121.742</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>test</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>howu</td>\n",
       "      <td>36.7085</td>\n",
       "      <td>-121.749</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>test</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540</th>\n",
       "      <td>hfvr</td>\n",
       "      <td>36.7962</td>\n",
       "      <td>-121.782</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>test</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17559</th>\n",
       "      <td>thki</td>\n",
       "      <td>36.7254</td>\n",
       "      <td>-121.730</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>test</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid  latitude  longitude       date split  year  month  week  season  \\\n",
       "6865   howw   37.0062   -120.600 2013-01-08  test  2013      1     2       1   \n",
       "3661   eamn   36.9818   -120.221 2013-01-08  test  2013      1     2       1   \n",
       "7668   imsv   36.9836   -120.500 2013-01-08  test  2013      1     2       1   \n",
       "20182  wgxq   33.8011   -117.205 2013-01-25  test  2013      1     4       1   \n",
       "16095  rsos   33.8892   -117.562 2013-01-25  test  2013      1     4       1   \n",
       "...     ...       ...        ...        ...   ...   ...    ...   ...     ...   \n",
       "12443  nsoi   36.7368   -121.734 2021-12-29  test  2021     12    52       1   \n",
       "14254  prfi   36.7518   -121.742 2021-12-29  test  2021     12    52       1   \n",
       "6864   howu   36.7085   -121.749 2021-12-29  test  2021     12    52       1   \n",
       "6540   hfvr   36.7962   -121.782 2021-12-29  test  2021     12    52       1   \n",
       "17559  thki   36.7254   -121.730 2021-12-29  test  2021     12    52       1   \n",
       "\n",
       "      region  severity  density  \n",
       "6865    west       NaN      NaN  \n",
       "3661    west       NaN      NaN  \n",
       "7668    west       NaN      NaN  \n",
       "20182   west       NaN      NaN  \n",
       "16095   west       NaN      NaN  \n",
       "...      ...       ...      ...  \n",
       "12443   west       NaN      NaN  \n",
       "14254   west       NaN      NaN  \n",
       "6864    west       NaN      NaN  \n",
       "6540    west       NaN      NaN  \n",
       "17559   west       NaN      NaN  \n",
       "\n",
       "[6510 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_data = test_data.sort_values(by='date')\n",
    "te_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14501, 12), (2559, 12))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data, val_data = train_test_split(train_data, test_size=0.15, random_state=123456789, shuffle=True)\n",
    "tr_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14501, 12), (2559, 12))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data2, val_data2 = train_test_split(train_data, test_size=0.15, random_state=123456789, shuffle=True)\n",
    "tr_data2.shape, val_data2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_2s = []\n",
    "g_from_past = {}\n",
    "\n",
    "n_times_called = {}\n",
    "\n",
    "def make_guess1(row: pd.Series, date=None, tr_data=tr_data) -> pd.Series:\n",
    "\n",
    "    dists = []\n",
    "    region = row.region\n",
    "    date = date\n",
    "    uid = row.uid\n",
    "    tr_data = tr_data\n",
    "\n",
    "    if date is None:\n",
    "        date = row.date\n",
    "\n",
    "    rel_data = tr_data[(tr_data.date == date) & (tr_data.region == region)]\n",
    "    \n",
    "    # check if cur date is past '2013-01-04'\n",
    "    if date < pd.to_datetime('2013-01-04'):\n",
    "        print(f'No previous data for this date filling in 2s .. for {row.uid}')\n",
    "        fill_2s.append(uid)\n",
    "        return 2\n",
    "\n",
    "    if rel_data.shape[0] == 0:\n",
    "        # print(f'No data for this date, trying previous day.. for {row.uid}')\n",
    "        if g_from_past.get(uid) is not None:\n",
    "            g_from_past[uid] += 1\n",
    "        return make_guess1(row, date=date - pd.Timedelta(days=1), tr_data=tr_data)\n",
    "\n",
    "    for some_row in rel_data.itertuples():\n",
    "        dist = get_distance(row.latitude, row.longitude, some_row.latitude, some_row.longitude)\n",
    "        dists.append(dist)\n",
    "    \n",
    "    nearest = rel_data.iloc[np.argmin(dists)]\n",
    "    return nearest.severity\n",
    "\n",
    "\n",
    "def make_guess2(row: pd.Series, date=None, tr_data=tr_data, n_times_called=None) -> pd.Series:\n",
    "    \"\"\"modified version of make_guess1, Uses mean/mode of severity for the region instead of nearest severity.\"\"\"\n",
    "    dists = []\n",
    "    region = row.region\n",
    "    date = date\n",
    "    uid = row.uid\n",
    "    tr_data = tr_data\n",
    "    \n",
    "    if n_times_called is not None:\n",
    "        if n_times_called.get(uid) is None:\n",
    "            n_times_called[uid] = 1\n",
    "        else:\n",
    "            n_times_called[uid] += 1\n",
    "        \n",
    "\n",
    "    if date is None:\n",
    "        date = row.date\n",
    "\n",
    "    rel_data = tr_data[(tr_data.date == date) & (tr_data.region == region)]\n",
    "    \n",
    "    # check if cur date is past '2013-01-04'\n",
    "    if date < pd.to_datetime('2013-01-04'):\n",
    "        print(f'No previous data for this date filling in 2s .. for {row.uid}')\n",
    "        fill_2s.append(uid)\n",
    "        return 2\n",
    "\n",
    "    if rel_data.shape[0] == 0:\n",
    "        # print(f'No data for this date, trying previous day.. for {row.uid}')\n",
    "        if g_from_past.get(uid) is not None:\n",
    "            g_from_past[uid] += 1\n",
    "        global count \n",
    "        count += 1\n",
    "        return make_guess2(row, date=date - pd.Timedelta(days=1), tr_data=tr_data)\n",
    "\n",
    "    severty_mode = rel_data.severity.mode()[0]\n",
    "    severty_mean = np.round(rel_data.severity.mean())\n",
    "\n",
    "    return severty_mean\n",
    "\n",
    "\n",
    "\n",
    "def cv_loop(rand, splits=10, guess_func=make_guess1):\n",
    "    # print(\"Random Number: \", rand)\n",
    "    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=rand)\n",
    "    tscv = TimeSeriesSplit(n_splits=splits)\n",
    "    \n",
    "    rmses = []\n",
    "    guess_train_preds = np.zeros((train_data.shape[0]))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(train_data, train_data.severity)):\n",
    "        print(f\"Fold: {fold}\")\n",
    "        tr_data, val_data = train_data.iloc[train_idx], train_data.iloc[val_idx]\n",
    "\n",
    "        val_data['guess'] = 0\n",
    "\n",
    "        temp = []\n",
    "        for row in tqdm(val_data.itertuples(), total=val_data.shape[0]):\n",
    "            uid_series = val_data[val_data.uid == row.uid]\n",
    "            severity = guess_func(uid_series.iloc[0], date=row.date, tr_data=tr_data)\n",
    "            val_data.loc[val_data.uid == row.uid, f'guess'] = severity\n",
    "            temp.append(severity)\n",
    "        \n",
    "        guess_train_preds[val_idx] = temp\n",
    "        \n",
    "\n",
    "                \n",
    "        errror = rmse(val_data.severity, val_data.guess1)\n",
    "        rmses.append(errror)\n",
    "        print(\"RMSE: \", errror)\n",
    "\n",
    "        print('Train Distribution: ')\n",
    "        print(tr_data.severity.value_counts(normalize=True))\n",
    "        print('Val Distribution: ')\n",
    "        print(val_data.severity.value_counts(normalize=True))\n",
    "        print('Predicted Distribution: ')\n",
    "        print(val_data.guess1.value_counts(normalize=True))\n",
    "\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "\n",
    "    return rmses, guess_train_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data.sort_values(by='date', inplace=True)\n",
    "val_data.sort_values(by='date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data2.sort_values(by='date', inplace=True)\n",
    "val_data2.sort_values(by='date', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed way of validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(val_data.itertuples(), total=val_data.shape[0]):\n",
    "    val_data.loc[row.Index, 'guess'] = make_guess1(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8516356696348804"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(val_data.severity, val_data.guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  0.6053145760062525\n",
      "Missed by 1:  0.3028526768268855\n",
      "Missed by 2:  0.08245408362641657\n",
      "Missed by 3:  0.008206330597889801\n",
      "Missed by 4:  0.0011723329425556857\n"
     ]
    }
   ],
   "source": [
    "analyize_matches(val_data.severity, val_data.guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.08284486127393513 + 0.008206330597889801 + 0.0011723329425556857\n",
    "# 91% preds < 1 offs, 99% preds < 2 offs, 88% preds == 2 offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2559/2559 [00:06<00:00, 414.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  0.5607659241891364\n",
      "Missed by 1:  0.3911684251660805\n",
      "Missed by 2:  0.04181320828448613\n",
      "Missed by 3:  0.005861664712778429\n",
      "Missed by 4:  0.00039077764751856197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7857663030948374"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for row in tqdm(val_data.itertuples(), total=val_data.shape[0]):\n",
    "    val_data.loc[row.Index, 'guess'] = make_guess2(row)\n",
    "\n",
    "analyize_matches(val_data.severity, val_data.guess)\n",
    "\n",
    "rmse(val_data.severity, val_data.guess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Era VAL SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data2['date_reg'] = val_data2.date.astype(str) + \"_\" +  val_data2.region\n",
    "tr_data2['date_reg'] = tr_data2.date.astype(str) + \"_\" +  tr_data2.region\n",
    "\n",
    "assert (val_data2.columns == tr_data2.columns).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(val_data2.uid).intersection(set(tr_data2.uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328793774319066"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Intersection percentage of date and regs before\n",
    "len(set(val_data2.date_reg).intersection(set(tr_data2.date_reg)))/val_data2.date_reg.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36867704280155644"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datereg_to_remove = val_data2.date_reg.sample(frac=0.40, random_state=123456789)\n",
    "tr_data2_ = tr_data2[~tr_data2.date_reg.isin(datereg_to_remove)]\n",
    "\n",
    "len(set(val_data2.date_reg).intersection(set(tr_data2_.date_reg)))/val_data2.date_reg.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6562107904642409"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(val_data2.date).intersection(set(tr_data2_.date)))/val_data2.date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 74/2559 [00:01<00:42, 57.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for jalu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 934/2559 [00:11<00:14, 114.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for jubi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1434/2559 [00:16<00:14, 79.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for pfly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1997/2559 [00:23<00:05, 108.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for qgrf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2559/2559 [00:27<00:00, 92.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  0.5224697147323173\n",
      "Missed by 1:  0.35248143806174287\n",
      "Missed by 2:  0.09183274716686206\n",
      "Missed by 3:  0.031652989449003514\n",
      "Missed by 4:  0.0015631105900742479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0147409034878858"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in tqdm(val_data2.itertuples(), total=val_data2.shape[0]):\n",
    "    val_data2.loc[row.Index, 'guess_2_new'] = make_guess2(row, tr_data=tr_data2_)\n",
    "\n",
    "analyize_matches(val_data2.severity, val_data2.guess_2_new)\n",
    "rmse(val_data2.severity, val_data2.guess_2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14501, 12)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2559, 14), (14501, 13))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data2.shape, tr_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2559, 14), (5617, 13))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  new data\n",
    "val_data2.shape, tr_data2_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze past submissions with new valdation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_sev_reg = pd.read_csv('../submissions/expndng_sev_by_reg_preds.csv')\n",
    "exp_sev_regweek = pd.read_csv('../submissions/expndng_sev_by_rw_exreg_preds.csv')\n",
    "exp_sev_rs = pd.read_csv('../submissions/expanding_sev_rs_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8939027540207913"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  exp_sev_reg\n",
    "\n",
    "#  past\n",
    "grp_by_region = data.groupby('region').severity.expanding(1).mean()\n",
    "grp_by_region = grp_by_region.map(np.round)\n",
    "\n",
    "grp_by_region['west'].fillna(2, inplace=True)\n",
    "grp_by_region['northeast'].fillna(2, inplace=True)\n",
    "print(grp_by_region.isna().sum())   # 5 --> 0.89416\n",
    "\n",
    "mse(train_data.severity.sort_index(), grp_by_region.droplevel(0).loc[train_data.index].sort_index(), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid             0\n",
       "latitude        0\n",
       "longitude       0\n",
       "date            0\n",
       "split           0\n",
       "year            0\n",
       "month           0\n",
       "week            0\n",
       "season          0\n",
       "region          0\n",
       "severity     2559\n",
       "density         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data2 = pd.concat([tr_data2, val_data2], axis=0)\n",
    "train_data2.sort_values(by='date', inplace=True)\n",
    "train_data2.loc[val_data2.index, 'severity'] = np.nan\n",
    "\n",
    "train_data2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9035123162781323"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_region = train_data2.groupby('region').severity.expanding(1).mean()\n",
    "grp_by_region = grp_by_region.map(np.round)\n",
    "\n",
    "grp_by_region['west'].fillna(2, inplace=True)\n",
    "grp_by_region['northeast'].fillna(2, inplace=True)\n",
    "print(grp_by_region.isna().sum())   # 5 --> 0.89416\n",
    "\n",
    "rmse(val_data2.severity.sort_index(), grp_by_region.droplevel(0).loc[val_data2.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.200169325034355"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_week = train_data2.groupby('week').severity.expanding(1).mean()\n",
    "grp_by_week = grp_by_week.map(np.round)\n",
    "print(grp_by_week.isna().sum())  # 105 --> 1.176284\n",
    "grp_by_week.fillna(2, inplace=True)\n",
    "\n",
    "rmse(val_data2.severity.sort_index(), grp_by_week.droplevel(0).loc[val_data2.index].sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.211673067179112"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_season = train_data2.groupby('season').severity.expanding(1).mean()\n",
    "grp_by_season = grp_by_season.map(np.round)\n",
    "print(grp_by_season.isna().sum())  # 105 --> 1.176284\n",
    "\n",
    "rmse(val_data2.severity.sort_index(), grp_by_season.droplevel(0).loc[val_data2.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2060159219778517"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_month = train_data2.groupby('month').severity.expanding(1).mean()\n",
    "grp_by_month = grp_by_month.map(np.round)\n",
    "print(grp_by_month.isna().sum())  # 105 --> 1.176284\n",
    "\n",
    "grp_by_month.fillna(2, inplace=True)\n",
    "\n",
    "rmse(val_data2.severity.sort_index(), grp_by_month.droplevel(0).loc[val_data2.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.224824635947432"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_region = train_data2.groupby('year').severity.expanding(1).mean()\n",
    "grp_by_region = grp_by_region.map(np.round)\n",
    "\n",
    "print(grp_by_region.isna().sum())   # 5 --> 0.89416\n",
    "grp_by_region.fillna(2, inplace=True)\n",
    "\n",
    "rmse(val_data2.severity.sort_index(), grp_by_region.droplevel(0).loc[val_data2.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 56\n",
      "uid                       0\n",
      "latitude                  0\n",
      "longitude                 0\n",
      "date                      0\n",
      "split                     0\n",
      "year                      0\n",
      "month                     0\n",
      "week                      0\n",
      "season                    0\n",
      "region                    0\n",
      "severity               2559\n",
      "density                   0\n",
      "expndng_sev_by_rw_1      56\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8856019075011943"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_rw = train_data2.groupby(['region', 'week']).severity.expanding(1).mean()\n",
    "grp_by_rw = grp_by_rw.map(np.round)\n",
    "print(\"Missing:\", grp_by_rw.isna().sum())  # 566 missing vaulues  --> 0.82\n",
    "\n",
    "train_data2['expndng_sev_by_rw_1'] = grp_by_rw.droplevel(0).droplevel(0).sort_index()\n",
    "\n",
    "\n",
    "# 1 for imputing missing values with expanding_avg_by_reg   # current best option\n",
    "# train_data2.expndng_sev_by_rw_1 = np.where(train_data2.expndng_sev_by_rw_1.isna(), train_data2.expndng_sev_by_reg, train_data2.expndng_sev_by_rw_1)\n",
    "grp_by_rw.fillna(2, inplace=True)\n",
    "\n",
    "print(train_data2.isna().sum())\n",
    "rmse(val_data2.severity.sort_index(), grp_by_rw.droplevel(0).droplevel(0).loc[val_data2.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Exact matches:  0.49628761234857366\n",
      "Missed by 1:  0.44001563110590075\n",
      "Missed by 2:  0.0523642047674873\n",
      "Missed by 3:  0.010550996483001172\n",
      "Missed by 4:  0.0007815552950371239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8700208636828513"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_rm = train_data2.groupby(['region', 'month']).severity.expanding(1).mean()\n",
    "grp_by_rm = grp_by_rm.map(np.round)\n",
    "print(grp_by_rm.isna().sum())  # 72 --> 0.8528\n",
    "\n",
    "grp_by_rm.fillna(2, inplace=True)\n",
    "analyize_matches(val_data2.severity.sort_index(), grp_by_rm.droplevel(0).droplevel(0).loc[val_data2.index].sort_index())\n",
    "rmse(val_data2.severity.sort_index(), grp_by_rm.droplevel(0).droplevel(0).loc[val_data2.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363032434544745"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.49628761234857366 + 0.44001563110590075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Exact matches:  0.5439624853458382\n",
      "Missed by 1:  0.365767878077374\n",
      "Missed by 2:  0.07542008597108245\n",
      "Missed by 3:  0.012895662368112544\n",
      "Missed by 4:  0.0019538882375928096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9026468828263917"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_rm = train_data2.groupby(['region', 'year']).severity.expanding(1).mean()\n",
    "grp_by_rm = grp_by_rm.map(np.round)\n",
    "print(grp_by_rm.isna().sum())  # 72 --> 0.8528\n",
    "\n",
    "grp_by_rm.fillna(2, inplace=True)\n",
    "\n",
    "analyize_matches(grp_by_rm.droplevel(0).droplevel(0).loc[val_data2.index].sort_index(), val_data2.severity.sort_index())\n",
    "rmse(val_data2.severity.sort_index(), grp_by_rm.droplevel(0).droplevel(0).loc[val_data2.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9097303634232122"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5439624853458382 + 0.365767878077374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Exact matches:  0.5001953888237592\n",
      "Missed by 1:  0.4361078546307151\n",
      "Missed by 2:  0.051191871824931616\n",
      "Missed by 3:  0.011332551778038297\n",
      "Missed by 4:  0.0011723329425556857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8727116562838365"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_rm = train_data2.groupby(['region', 'season']).severity.expanding(1).mean()\n",
    "grp_by_rm = grp_by_rm.map(np.round)\n",
    "print(grp_by_rm.isna().sum())  # 72 --> 0.8528\n",
    "\n",
    "grp_by_rm.fillna(2, inplace=True)\n",
    "\n",
    "analyize_matches(grp_by_rm.droplevel(0).droplevel(0).loc[val_data2.index].sort_index(), val_data2.severity.sort_index())\n",
    "\n",
    "rmse(val_data2.severity.sort_index(), grp_by_rm.droplevel(0).droplevel(0).loc[val_data2.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363032434544744"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5001953888237592 + 0.4361078546307151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736\n",
      "Exact matches:  0.4404064087534193\n",
      "Missed by 1:  0.4497850722938648\n",
      "Missed by 2:  0.10042985541227042\n",
      "Missed by 3:  0.008987885892926924\n",
      "Missed by 4:  0.00039077764751856197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9688384330421589"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_rm = train_data2.groupby(['region', 'date']).severity.expanding(1).mean()\n",
    "grp_by_rm = grp_by_rm.map(np.round)\n",
    "print(grp_by_rm.isna().sum())  # 72 --> 0.8528\n",
    "\n",
    "grp_by_rm.fillna(2, inplace=True)\n",
    "\n",
    "preds = grp_by_rm.droplevel(0).droplevel(0).loc[val_data2.index].sort_index()\n",
    "\n",
    "analyize_matches(val_data2.severity.sort_index(), preds)\n",
    "\n",
    "rmse(val_data2.severity.sort_index(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8901914810472841"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4404064087534193 + 0.4497850722938648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6575758322631906,\n",
       " 1.2082819584556195,\n",
       " 1.4738836258523553,\n",
       " 2.2101404014184314,\n",
       " 3.0979199869280616)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(val_data2.severity, [1]*len(val_data2)), rmse(val_data2.severity, [2]*len(val_data2)), rmse(val_data2.severity, [3]*len(val_data2)), rmse(val_data2.severity, [4]*len(val_data2)), rmse(val_data2.severity, [5]*len(val_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  so one more observation is : samples missed by <=1 should be less greatere than 90% inorder to breach rmse 0.89 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 76/2559 [00:01<00:29, 83.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for jalu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 936/2559 [00:12<00:17, 94.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for jubi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1430/2559 [00:18<00:11, 97.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for pfly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1992/2559 [00:24<00:06, 91.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for qgrf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2559/2559 [00:29<00:00, 85.35it/s] \n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(val_data2.itertuples(), total=val_data2.shape[0]):\n",
    "    val_data2.loc[row.Index, 'guess_1_new'] = make_guess1(row, tr_data=tr_data2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  0.5447440406408753\n",
      "Missed by 1:  0.29269245799140287\n",
      "Missed by 2:  0.1250488472059398\n",
      "Missed by 3:  0.03673309886674482\n",
      "Missed by 4:  0.0007815552950371239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0658286078617234"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyize_matches(val_data2.severity, val_data2.guess_1_new)\n",
    "\n",
    "rmse(val_data2.severity, val_data2.guess_1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  So guess1 should also give 1.06 or something like that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 88/6510 [00:01<01:09, 92.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for igpa\n",
      "No previous data for this date filling in 2s .. for lkpf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 106/6510 [00:02<02:27, 43.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data for this date filling in 2s .. for paez\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6510/6510 [01:17<00:00, 83.95it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabn</td>\n",
       "      <td>west</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aair</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aajw</td>\n",
       "      <td>northeast</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalr</td>\n",
       "      <td>midwest</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aalw</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>zzpn</td>\n",
       "      <td>northeast</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>zzrv</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>zzsx</td>\n",
       "      <td>south</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6509</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid     region  severity\n",
       "0     aabn       west         2\n",
       "1     aair       west         4\n",
       "2     aajw  northeast         2\n",
       "3     aalr    midwest         4\n",
       "4     aalw       west         4\n",
       "...    ...        ...       ...\n",
       "6505  zzpn  northeast         5\n",
       "6506  zzrv       west         4\n",
       "6507  zzsx      south         2\n",
       "6508  zzvv       west         4\n",
       "6509  zzzi    midwest         1\n",
       "\n",
       "[6510 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Making submission with guess2 mean\n",
    "\n",
    "sub_format['severity'] = 0\n",
    "\n",
    "for row in tqdm(te_data.itertuples(), total=te_data.shape[0]):\n",
    "    uid_series = te_data[te_data.uid == row.uid]\n",
    "    severity = make_guess1(uid_series.iloc[0], date=row.date, tr_data=train_data)   # use all train data for making test submission\n",
    "    sub_format.loc[sub_format.uid == row.uid, 'severity'] = severity\n",
    "\n",
    "sub_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    0.339785\n",
      "1    0.311982\n",
      "3    0.191705\n",
      "2    0.153303\n",
      "5    0.003226\n",
      "Name: severity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(sub_format.severity.value_counts(normalize=True))\n",
    "\n",
    "sub_format.to_csv('../submissions/to submit/guess1_newly_validated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sooo....\n",
    "\n",
    "- The discrepancy is due to high percentage of dates and region combinations (as in myguess2) is missing in the test set, where as the val set is so sweet and nice and has all the    combinations\n",
    "- finally figured out something, but not sure if it's right.\n",
    "- \n",
    "\n",
    "# Todos :\n",
    "\n",
    "- check this theory with a submission.\n",
    "- analyise date missses vs date_reg misses.\n",
    "- Figure out better hypothesis guessing on new val set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "186db977f413ae6598820b658aed1651932768522d0652c07d949b3f5fb38b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
