{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Mission : Error Analysis & Model metadata``\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from geopy.distance import geodesic\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('../data/metadata.csv')\n",
    "sub_format = pd.read_csv('../data/submission_format.csv')\n",
    "train_labels = pd.read_csv('../data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mse(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dens_to_sev(x: float)-> int:\n",
    "    \"\"\"takes in density value in cells/ml and returns severity category\"\"\"\n",
    "    if (x < 20_000) : return 1\n",
    "    elif (x >= 20_000) and (x < 100_000) : return 2\n",
    "    elif (x >= 100_000) and (x < 1_000_000) : return 3\n",
    "    elif (x >= 1_000_000) and (x < 10_000_000) : return 4\n",
    "    elif x > 10_000_000 : return 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add date fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabm</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>train</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>585.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aacd</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>train</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaee</td>\n",
       "      <td>35.487000</td>\n",
       "      <td>-79.062133</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>train</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaff</td>\n",
       "      <td>38.049471</td>\n",
       "      <td>-99.827001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>train</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>3.0</td>\n",
       "      <td>111825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23566</th>\n",
       "      <td>zzwo</td>\n",
       "      <td>39.792190</td>\n",
       "      <td>-99.971050</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>train</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23567</th>\n",
       "      <td>zzwq</td>\n",
       "      <td>35.794000</td>\n",
       "      <td>-79.012551</td>\n",
       "      <td>2015-03-24</td>\n",
       "      <td>train</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23568</th>\n",
       "      <td>zzyb</td>\n",
       "      <td>35.742000</td>\n",
       "      <td>-79.238600</td>\n",
       "      <td>2016-11-21</td>\n",
       "      <td>train</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23570 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date  split  year  month  week  \\\n",
       "0      aabm  39.080319  -86.430867 2018-05-14  train  2018      5    20   \n",
       "1      aabn  36.559700 -121.510000 2016-08-31   test  2016      8    35   \n",
       "2      aacd  35.875083  -78.878434 2020-11-19  train  2020     11    47   \n",
       "3      aaee  35.487000  -79.062133 2016-08-24  train  2016      8    34   \n",
       "4      aaff  38.049471  -99.827001 2019-07-23  train  2019      7    30   \n",
       "...     ...        ...         ...        ...    ...   ...    ...   ...   \n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02   test  2014     12    49   \n",
       "23566  zzwo  39.792190  -99.971050 2017-06-19  train  2017      6    25   \n",
       "23567  zzwq  35.794000  -79.012551 2015-03-24  train  2015      3    13   \n",
       "23568  zzyb  35.742000  -79.238600 2016-11-21  train  2016     11    47   \n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31   test  2015      8    36   \n",
       "\n",
       "       season   region  severity   density  \n",
       "0           2  midwest       1.0     585.0  \n",
       "1           3     west       NaN       NaN  \n",
       "2           4    south       1.0     290.0  \n",
       "3           3    south       1.0    1614.0  \n",
       "4           3  midwest       3.0  111825.0  \n",
       "...       ...      ...       ...       ...  \n",
       "23565       1     west       NaN       NaN  \n",
       "23566       3  midwest       2.0   48510.0  \n",
       "23567       2    south       1.0    1271.0  \n",
       "23568       4    south       1.0    9682.0  \n",
       "23569       3  midwest       NaN       NaN  \n",
       "\n",
       "[23570 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.date = pd.to_datetime(metadata.date)\n",
    "metadata['year'] = metadata.date.dt.year\n",
    "metadata['month'] = metadata.date.dt.month\n",
    "metadata['week'] = metadata.date.dt.isocalendar().week\n",
    "\n",
    "\n",
    "seasons = {\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "    6: 3,\n",
    "    7: 3,\n",
    "    8: 3,\n",
    "    9: 4,\n",
    "    10: 4,\n",
    "    11: 4,\n",
    "    12: 1\n",
    "}\n",
    "\n",
    "metadata['season'] = metadata.month.map(seasons)\n",
    "\n",
    "\n",
    "region = pd.concat((train_labels, sub_format[['region', 'uid']]), axis=0)\n",
    "\n",
    "data = pd.merge(metadata, region, on='uid', how='left')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6510, 12), (23570, 12))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = data[data.split == 'test']\n",
    "test_data.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17060, 12), (23570, 12))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data[data.split == 'train']\n",
    "train_data.shape, data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Utils\n",
    "def get_data_by_date( date=None, data=train_data):\n",
    "    return data[data.date == date]\n",
    "\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "\n",
    "def analyize_matches(y_true, y_pred):\n",
    "    print(\"Exact matches: \", sum(y_true == y_pred) / len(y_true))\n",
    "    \n",
    "    print(\"Missed by 1: \", sum(abs(y_true - y_pred) == 1) / len(y_true))\n",
    "    print(\"Missed by 2: \", sum(abs(y_true - y_pred) == 2) / len(y_true))\n",
    "    print(\"Missed by 3: \", sum(abs(y_true - y_pred) == 3) / len(y_true))\n",
    "    print(\"Missed by 4: \", sum(abs(y_true - y_pred) == 4) / len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_data = test_data.sort_values(by='date')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14501, 12), (2559, 12))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data, val_data = train_test_split(train_data, test_size=0.15, random_state=123456789, shuffle=True)\n",
    "tr_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      " % of intersection between date and regions in val and train sets before correction: 0.9328793774319066\n",
      " % of intersection between date and regions in val and train sets after correction: 0.36867704280155644\n",
      " % of intersection between dates in val and train sets: 0.656211                                                                    \n",
      " % of intersection between date in test and train sets: 0.701095                                                                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5617, 13), (2559, 13), (17060, 12), (2559, 13))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data similar to test data\n",
    "val_data['date_reg'] = val_data.date.astype(str) + \"_\" +  val_data.region\n",
    "tr_data['date_reg'] = tr_data.date.astype(str) + \"_\" +  tr_data.region\n",
    "\n",
    "assert (val_data.columns == tr_data.columns).all()\n",
    "\n",
    "print(set(val_data.uid).intersection(set(tr_data.uid)))\n",
    "\n",
    "# percentage of intersection date and regs before\n",
    "print(f\" % of intersection between date and regions in val and train sets before correction: {len(set(val_data.date_reg).intersection(set(tr_data.date_reg)))/val_data.date_reg.nunique()}\")\n",
    "\n",
    "datereg_to_remove = val_data.date_reg.sample(frac=0.40, random_state=123456789)\n",
    "tr_data2_te_dist = tr_data[~tr_data.date_reg.isin(datereg_to_remove)]\n",
    "val_data2_te_dist = val_data\n",
    "\n",
    "print(f\" % of intersection between date and regions in val and train sets after correction: {len(set(val_data2_te_dist.date_reg).intersection(set(tr_data2_te_dist.date_reg)))/val_data2_te_dist.date_reg.nunique()}\")\n",
    "\n",
    "\n",
    "print(f\" % of intersection between dates in val and train sets: {len(set(val_data2_te_dist.date).intersection(set(tr_data2_te_dist.date)))/val_data2_te_dist.date.nunique() :<75f} \")\n",
    "print(f\" % of intersection between date in test and train sets: {len(set(test_data.date).intersection(set(train_data.date)))/test_data.date.nunique():<75f}\" )\n",
    "\n",
    "tr_data2_te_dist.shape, val_data2_te_dist.shape, train_data.shape, val_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching dateregs btw val and tr: 0.4\n",
      "matching dates btw val and tr: 0.7589285714285714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14501, 13), (131, 13))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data3 = tr_data.copy()\n",
    "val_data3 = val_data.copy()\n",
    "\n",
    "datereg_to_remove = tr_data3.date_reg.sample(frac=0.60, random_state=123456789)\n",
    "val_data3_te_dist = val_data3[~val_data3.date_reg.isin(datereg_to_remove)]\n",
    "tr_data3_te_dist = tr_data3\n",
    "\n",
    "print(\"matching dateregs btw val and tr:\", len(set(val_data3_te_dist.date_reg).intersection(set(tr_data3_te_dist.date_reg)))/val_data3_te_dist.date_reg.nunique())\n",
    "print(\"matching dates btw val and tr:\",len(set(val_data3_te_dist.date).intersection(set(tr_data3_te_dist.date)))/val_data3_te_dist.date.nunique())\n",
    "\n",
    "tr_data3_te_dist.shape, val_data3_te_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11968, 12), (5092, 12))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  split by time\n",
    "train_data = train_data.sort_values(by='date')\n",
    "train_data_ts = train_data[train_data.date < '2019-01-01']\n",
    "val_data_ts = train_data[train_data.date >= '2019-01-01']\n",
    "\n",
    "train_data_ts.shape, val_data_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_2s = []\n",
    "g_from_past = {}\n",
    "\n",
    "n_times_called = {}\n",
    "\n",
    "def make_guess1(row: pd.Series, date=None, tr_data=tr_data) -> pd.Series:\n",
    "\n",
    "    dists = []\n",
    "    region = row.region\n",
    "    date = date\n",
    "    uid = row.uid\n",
    "    tr_data = tr_data\n",
    "\n",
    "    if date is None:\n",
    "        date = row.date\n",
    "\n",
    "    rel_data = tr_data[(tr_data.date == date) & (tr_data.region == region)]\n",
    "    \n",
    "    # check if cur date is past '2013-01-04'\n",
    "    if date < pd.to_datetime('2013-01-04'):\n",
    "        print(f'No previous data for this date filling in 2s .. for {row.uid}')\n",
    "        fill_2s.append(uid)\n",
    "        return 2\n",
    "\n",
    "    if rel_data.shape[0] == 0:\n",
    "        # print(f'No data for this date, trying previous day.. for {row.uid}')\n",
    "        if g_from_past.get(uid) is not None:\n",
    "            g_from_past[uid] += 1\n",
    "        return make_guess1(row, date=date - pd.Timedelta(days=1), tr_data=tr_data)\n",
    "\n",
    "    for some_row in rel_data.itertuples():\n",
    "        dist = get_distance(row.latitude, row.longitude, some_row.latitude, some_row.longitude)\n",
    "        dists.append(dist)\n",
    "    \n",
    "    nearest = rel_data.iloc[np.argmin(dists)]\n",
    "    return nearest.severity\n",
    "\n",
    "\n",
    "def make_guess2(row: pd.Series, date=None, tr_data=tr_data, n_times_called=None) -> pd.Series:\n",
    "    \"\"\"modified version of make_guess1, Uses mean/mode of severity for the region instead of nearest severity.\"\"\"\n",
    "    dists = []\n",
    "    region = row.region\n",
    "    date = date\n",
    "    uid = row.uid\n",
    "    tr_data = tr_data\n",
    "    \n",
    "    if n_times_called is not None:\n",
    "        if n_times_called.get(uid) is None:\n",
    "            n_times_called[uid] = 1\n",
    "        else:\n",
    "            n_times_called[uid] += 1\n",
    "        \n",
    "\n",
    "    if date is None:\n",
    "        date = row.date\n",
    "\n",
    "    rel_data = tr_data[(tr_data.date == date) & (tr_data.region == region)]\n",
    "    \n",
    "    # check if cur date is past '2013-01-04'\n",
    "    if date < pd.to_datetime('2013-01-04'):\n",
    "        print(f'No previous data for this date filling in 2s .. for {row.uid}')\n",
    "        fill_2s.append(uid)\n",
    "        return 2\n",
    "\n",
    "    if rel_data.shape[0] == 0:\n",
    "        # print(f'No data for this date, trying previous day.. for {row.uid}')\n",
    "        if g_from_past.get(uid) is not None:\n",
    "            g_from_past[uid] += 1\n",
    "        global count \n",
    "        count += 1\n",
    "        return make_guess2(row, date=date - pd.Timedelta(days=1), tr_data=tr_data)\n",
    "\n",
    "    severty_mode = rel_data.severity.mode()[0]\n",
    "    severty_mean = np.round(rel_data.severity.mean())\n",
    "\n",
    "    return severty_mean\n",
    "\n",
    "\n",
    "\n",
    "def cv_loop(rand, splits=10, guess_func=make_guess1):\n",
    "    # print(\"Random Number: \", rand)\n",
    "    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=rand)\n",
    "    tscv = TimeSeriesSplit(n_splits=splits)\n",
    "    \n",
    "    rmses = []\n",
    "    guess_train_preds = np.zeros((train_data.shape[0]))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(train_data, train_data.severity)):\n",
    "        print(f\"Fold: {fold}\")\n",
    "        tr_data, val_data = train_data.iloc[train_idx], train_data.iloc[val_idx]\n",
    "\n",
    "        val_data['guess'] = 0\n",
    "\n",
    "        temp = []\n",
    "        for row in tqdm(val_data.itertuples(), total=val_data.shape[0]):\n",
    "            uid_series = val_data[val_data.uid == row.uid]\n",
    "            severity = guess_func(uid_series.iloc[0], date=row.date, tr_data=tr_data)\n",
    "            val_data.loc[val_data.uid == row.uid, f'guess'] = severity\n",
    "            temp.append(severity)\n",
    "        \n",
    "        guess_train_preds[val_idx] = temp\n",
    "        \n",
    "\n",
    "                \n",
    "        errror = rmse(val_data.severity, val_data.guess1)\n",
    "        rmses.append(errror)\n",
    "        print(\"RMSE: \", errror)\n",
    "\n",
    "        print('Train Distribution: ')\n",
    "        print(tr_data.severity.value_counts(normalize=True))\n",
    "        print('Val Distribution: ')\n",
    "        print(val_data.severity.value_counts(normalize=True))\n",
    "        print('Predicted Distribution: ')\n",
    "        print(val_data.guess1.value_counts(normalize=True))\n",
    "\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "\n",
    "    return rmses, guess_train_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data.sort_values(by='date', inplace=True)\n",
    "val_data.sort_values(by='date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to clip values and trees are not good at extrapolation!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guess based on nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region\n",
       "midwest      2.194091\n",
       "northeast    1.805774\n",
       "south        1.567652\n",
       "west         3.747413\n",
       "Name: severity, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('region').severity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_sev_map = {\n",
    "    'midwest': 2,\n",
    "    'northeast': 2,\n",
    "    'south' : 2,\n",
    "    'west' : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(row=None, train_data=tr_data, k=1):\n",
    "    \"\"\"\n",
    "    row : pd.Series (row from val_data)\n",
    "    train_data : pd.DataFrame (all_data)\n",
    "    k : int number of nearest neighbours to consider\n",
    "    \n",
    "    algo:\n",
    "    1. Get past month data collected till the current row\n",
    "    2. Get the k nearest neighbours (geodesic dist using lat, lng) from the above data\n",
    "    3. Get the mean of the severity from the above rows\n",
    "    4. Return the mean of the nearest neighbours severity\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if row is None:\n",
    "        print('Row None bruv!')\n",
    "        return None\n",
    "    \n",
    "    uid = row.uid\n",
    "    date = row.date\n",
    "    region = row.region\n",
    "    past_date = date - pd.Timedelta(days=30)\n",
    "    \n",
    "    past_month_data = train_data[(train_data.date < date) & (train_data.date >= past_date)]\n",
    "    past_month_data.sort_values(by='date', inplace=True)\n",
    "    \n",
    "    # if no past data, return the mean of the region\n",
    "    if len(past_month_data) == 0:\n",
    "        return reg_sev_map[region]\n",
    "\n",
    "    \n",
    "    dist_matrix =pd.DataFrame(columns=['uid', 'dist'])       # 0th col for uid, 1st col for dist\n",
    "    for i, past_row in enumerate(past_month_data.itertuples()):\n",
    "        dist_matrix.loc[i, 'uid'] = past_row.uid\n",
    "        dist_matrix.loc[i, 'dist'] = get_distance(row.latitude, row.longitude, past_row.latitude, past_row.longitude)  # returns geodesic dist in km\n",
    "\n",
    "    # get mean of top k nearest neighbours\n",
    "    n_uids = dist_matrix.sort_values(by='dist').head(k).uid.values\n",
    "    nn_severity = train_data[train_data.uid.isin(n_uids)].severity.mean()\n",
    "    \n",
    "    return np.round(nn_severity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.uid.isin(tr_data.uid).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14501, 13), (5617, 13), (14501, 13))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.shape, tr_data2_te_dist.shape, tr_data3_te_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_wrapper(k=1, train_data=train_data, val_data=val_data):\n",
    "    sev_list = Parallel(n_jobs=-1, backend='loky')([delayed(knn)(row, train_data=train_data, k=k) for row in val_data.itertuples()])\n",
    "    return sev_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 28s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#  iterate over val_data and get the severity for each row\n",
    "\n",
    "k = 20\n",
    "val_data_20 = knn_wrapper(k=k, train_data=tr_data, val_data=val_data)  # random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9428781194325851"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(val_data.severity, val_data_20)\n",
    "# k=1  0.9511310393449847\n",
    "# k=5  0.8807347105254485\n",
    "# k=20 0.9428781194325851\n",
    "# k=30 0.9951032905241727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=1  0.9515418063860925\n",
    "# k=2  0.9511310393449847\n",
    "# k=3  0.9076118558993864\n",
    "# k=4  0.8820647913897578\n",
    "# k=5  0.8738303744155784\n",
    "# k=6  0.8884654455787772\n",
    "# k=7  0.8873651783510002\n",
    "# k=8  0.8853812517340963\n",
    "# k=9  0.894165010958815\n",
    "# k=10 0.8965654582604005\n",
    "# k=30 0.9951032905241727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data2_te_dist.uid.isin(tr_data2_te_dist.uid).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15685195376995048"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data2_te_dist['latlng'] = tr_data2_te_dist.latitude.astype(str) + \"_\" + tr_data2_te_dist.longitude.astype(str)\n",
    "val_data2_te_dist['latlng'] = val_data2_te_dist.latitude.astype(str) + \"_\" + val_data2_te_dist.longitude.astype(str)\n",
    "len(set(tr_data2_te_dist.latlng).intersection(set(val_data2_te_dist.latlng)))/len(set(val_data2_te_dist.latlng))\n",
    "# 15% intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0093350915731063"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "val_data2_te_5 = knn_wrapper(k=k, train_data=tr_data2_te_dist, val_data=val_data2_te_dist)\n",
    "rmse(val_data2_te_dist.severity, val_data2_te_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2245055465487782"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k=1  1.0176250615645182\n",
    "# k=5  1.01088\n",
    "# k=10 1.075500737635704\n",
    "# k=15 1.121915684290619\n",
    "# k=30 1.2245055465487782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  0.5033216100039077\n",
      "Missed by 1:  0.365767878077374\n",
      "Missed by 2:  0.10668229777256741\n",
      "Missed by 3:  0.023055881203595155\n",
      "Missed by 4:  0.0011723329425556857\n"
     ]
    }
   ],
   "source": [
    "analyize_matches(val_data2_te_dist.severity, val_data2_te_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8690894880812817"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.5033216100039077,  0.365767878077374, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  0.4650254005470887\n",
      "Missed by 1:  0.39663931223134036\n",
      "Missed by 2:  0.09808518952715904\n",
      "Missed by 3:  0.03946854239937476\n",
      "Missed by 4:  0.0007815552950371239\n"
     ]
    }
   ],
   "source": [
    "analyize_matches(val_data2_te_dist.severity, val_data2_te_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  0.31652989449003516\n",
      "Missed by 1:  0.5013677217663149\n",
      "Missed by 2:  0.1281750683860883\n",
      "Missed by 3:  0.053927315357561546\n",
      "Missed by 4:  0.0\n"
     ]
    }
   ],
   "source": [
    "analyize_matches(val_data2_te_dist.severity, val_data2_te_dist['nn30_guess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  0.5513872606486909\n",
      "Missed by 1:  0.30754200859710823\n",
      "Missed by 2:  0.10941774130519734\n",
      "Missed by 3:  0.03087143415396639\n",
      "Missed by 4:  0.0007815552950371239\n"
     ]
    }
   ],
   "source": [
    "analyize_matches(val_data2_te_dist.severity, val_data2_te_dist['nn1_guess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5754716981132075"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data3_te_dist['latlng'] = tr_data3_te_dist.latitude.astype(str) + \"_\" + tr_data3_te_dist.longitude.astype(str)\n",
    "val_data3_te_dist['latlng'] =val_data3_te_dist.latitude.astype(str) + \"_\" +val_data3_te_dist.longitude.astype(str)\n",
    "\n",
    "len(set(tr_data3_te_dist.latlng).intersection(set(val_data3_te_dist.latlng)))/len(set(val_data3_te_dist.latlng))\n",
    "# 60% intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:14<00:00,  9.14it/s]\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "for row in tqdm(val_data3_te_dist.itertuples(), total=val_data3_te_dist.shape[0]):\n",
    "    severity = knn(row, train_data=tr_data3, k=k)\n",
    "    val_data3_te_dist.loc[val_data3_te_dist.uid == row.uid, f'nn{k}_guess'] = severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.066491715932511"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(val_data3_te_dist.severity,val_data3_te_dist.nn30_guess)\n",
    "# 30 1.066\n",
    "# 15 1.04\n",
    "# 5 0.977\n",
    "# 1 1.059"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exapanding means again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by='date')\n",
    "test_data_sorted = test_data.sort_values(by='date')\n",
    "\n",
    "all_train_data_sorted = train_data.sort_values(by='date')\n",
    "train_data_sorted = tr_data.sort_values(by='date')\n",
    "val_data_sorted = val_data.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9015639227689898"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by_region = train_data.groupby('region').severity.expanding(1).mean()\n",
    "grp_by_region = grp_by_region.map(np.round)\n",
    "\n",
    "grp_by_region['west'].fillna(4, inplace=True)\n",
    "grp_by_region['northeast'].fillna(4, inplace=True)\n",
    "print(grp_by_region.isna().sum())\n",
    "\n",
    "# rmse \n",
    "rmse(val_data[val_data.index.isin(train_data.index)].severity.sort_index(), grp_by_region.droplevel(0).loc[val_data.index].sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>date_reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>gdxr</td>\n",
       "      <td>44.877646</td>\n",
       "      <td>-93.557842</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13644</th>\n",
       "      <td>paev</td>\n",
       "      <td>44.822478</td>\n",
       "      <td>-93.367962</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>evep</td>\n",
       "      <td>44.847993</td>\n",
       "      <td>-93.476318</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>fwbt</td>\n",
       "      <td>44.850500</td>\n",
       "      <td>-93.515700</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>guny</td>\n",
       "      <td>44.878889</td>\n",
       "      <td>-93.490833</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude  longitude       date  split  year  month  week  \\\n",
       "5566   gdxr  44.877646 -93.557842 2013-01-04  train  2013      1     1   \n",
       "13644  paev  44.822478 -93.367962 2013-01-04  train  2013      1     1   \n",
       "4387   evep  44.847993 -93.476318 2013-01-04  train  2013      1     1   \n",
       "5317   fwbt  44.850500 -93.515700 2013-01-04  train  2013      1     1   \n",
       "6144   guny  44.878889 -93.490833 2013-01-04  train  2013      1     1   \n",
       "\n",
       "       season   region  severity  density            date_reg  \n",
       "5566        1  midwest       1.0   1416.0  2013-01-04_midwest  \n",
       "13644       1  midwest       1.0   1884.0  2013-01-04_midwest  \n",
       "4387        1  midwest       1.0    115.0  2013-01-04_midwest  \n",
       "5317        1  midwest       1.0    476.0  2013-01-04_midwest  \n",
       "6144        1  midwest       1.0    558.0  2013-01-04_midwest  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = pd.concat([tr_data3_te_dist, val_data3_te_dist])\n",
    "data3 = data3.sort_values(by='date')\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>date_reg</th>\n",
       "      <th>nn30_guess</th>\n",
       "      <th>nn20_guess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>gdxr</td>\n",
       "      <td>44.877646</td>\n",
       "      <td>-93.557842</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>evep</td>\n",
       "      <td>44.847993</td>\n",
       "      <td>-93.476318</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>guny</td>\n",
       "      <td>44.878889</td>\n",
       "      <td>-93.490833</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13644</th>\n",
       "      <td>paev</td>\n",
       "      <td>44.822478</td>\n",
       "      <td>-93.367962</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>fwbt</td>\n",
       "      <td>44.850500</td>\n",
       "      <td>-93.515700</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>train</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>2013-01-04_midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude  longitude       date  split  year  month  week  \\\n",
       "5566   gdxr  44.877646 -93.557842 2013-01-04  train  2013      1     1   \n",
       "4387   evep  44.847993 -93.476318 2013-01-04  train  2013      1     1   \n",
       "6144   guny  44.878889 -93.490833 2013-01-04  train  2013      1     1   \n",
       "13644  paev  44.822478 -93.367962 2013-01-04  train  2013      1     1   \n",
       "5317   fwbt  44.850500 -93.515700 2013-01-04  train  2013      1     1   \n",
       "\n",
       "       season   region  severity  density            date_reg  nn30_guess  \\\n",
       "5566        1  midwest       1.0   1416.0  2013-01-04_midwest         NaN   \n",
       "4387        1  midwest       1.0    115.0  2013-01-04_midwest         NaN   \n",
       "6144        1  midwest       1.0    558.0  2013-01-04_midwest         NaN   \n",
       "13644       1  midwest       1.0   1884.0  2013-01-04_midwest         NaN   \n",
       "5317        1  midwest       1.0    476.0  2013-01-04_midwest         NaN   \n",
       "\n",
       "       nn20_guess  \n",
       "5566          NaN  \n",
       "4387          NaN  \n",
       "6144          NaN  \n",
       "13644         NaN  \n",
       "5317          NaN  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.concat([tr_data2_te_dist, val_data2_te_dist], axis=0)\n",
    "data2 = data2.sort_values(by='date')\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Exact matches:  0.46658851113716293\n",
      "Missed by 1:  0.4697147323173114\n",
      "Missed by 2:  0.047674872997264556\n",
      "Missed by 3:  0.016021883548261038\n",
      "Missed by 4:  0.0\n",
      "val rmse 0.8970012130653553\n",
      "trian rmse 0.925423687808426\n"
     ]
    }
   ],
   "source": [
    "grp_by_region = data2.groupby('region').severity.expanding(1).mean()\n",
    "grp_by_region = grp_by_region.map(np.round)\n",
    "\n",
    "grp_by_region['west'].fillna(4, inplace=True)\n",
    "grp_by_region['northeast'].fillna(4, inplace=True)\n",
    "print(grp_by_region.isna().sum())\n",
    "\n",
    "\n",
    "analyize_matches(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_region.droplevel(0).loc[val_data2_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_region.droplevel(0).loc[val_data2_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data2[data2.index.isin(data2.index)].severity.sort_index(), grp_by_region.droplevel(0).loc[data2.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Exact matches:  0.4618991793669402\n",
      "Missed by 1:  0.4802657288003126\n",
      "Missed by 2:  0.0492379835873388\n",
      "Missed by 3:  0.007424775302852677\n",
      "Missed by 4:  0.0011723329425556857\n",
      "val rmse 0.8733830591190974\n",
      "trian rmse 0.8923188201042993\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data2.groupby(['region', 'season']).severity.expanding(1).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "print(grp_by_rs.isna().sum())\n",
    "\n",
    "\n",
    "analyize_matches(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data2[data2.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data2.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why the heck nan != nan ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Exact matches:  0.45720984759671746\n",
      "Missed by 1:  0.4802657288003126\n",
      "Missed by 2:  0.0492379835873388\n",
      "Missed by 3:  0.007424775302852677\n",
      "Missed by 4:  0.0011723329425556857\n",
      "val rmse 0.8749706353000126\n",
      "trian rmse 0.8939226593032625\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data2.groupby(['region', 'season']).severity.expanding(3).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "print(grp_by_rs.isna().sum())\n",
    "# grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "# filling in nan with mean of region\n",
    "grp_by_rs.fillna(grp_by_rs.groupby('region').transform('mean'), inplace=True)\n",
    "\n",
    "analyize_matches(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data2[data2.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data2.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Exact matches:  0.45955451348182885\n",
      "Missed by 1:  0.4802657288003126\n",
      "Missed by 2:  0.0492379835873388\n",
      "Missed by 3:  0.007424775302852677\n",
      "Missed by 4:  0.0011723329425556857\n",
      "val rmse 0.8738803395059542\n",
      "trian rmse 0.8932731896924032\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data2.groupby(['region', 'season']).severity.expanding(2).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "print(grp_by_rs.isna().sum())\n",
    "# grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "# filling in nan with mean of region\n",
    "grp_by_rs.fillna(grp_by_rs.groupby('region').transform('mean'), inplace=True)\n",
    "\n",
    "analyize_matches(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data2[data2.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data2.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Exact matches:  0.4544744040640875\n",
      "Missed by 1:  0.4794841735052755\n",
      "Missed by 2:  0.0492379835873388\n",
      "Missed by 3:  0.007424775302852677\n",
      "Missed by 4:  0.0011723329425556857\n",
      "val rmse 0.8755030384007657\n",
      "trian rmse 0.8943272674588092\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data2.groupby(['region', 'season']).severity.expanding(5).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "print(grp_by_rs.isna().sum())\n",
    "# grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "# filling in nan with mean of region\n",
    "grp_by_rs.fillna(grp_by_rs.groupby('region').transform('mean'), inplace=True)\n",
    "\n",
    "analyize_matches(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data2[data2.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data2.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n",
      "Exact matches:  0.4282923016803439\n",
      "Missed by 1:  0.4618991793669402\n",
      "Missed by 2:  0.04572098475967175\n",
      "Missed by 3:  0.005861664712778429\n",
      "Missed by 4:  0.0007815552950371239\n",
      "val rmse 0.8714837174049794\n",
      "trian rmse 0.8960564303740658\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data2.groupby(['region', 'season']).severity.expanding(50).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "print(grp_by_rs.isna().sum())\n",
    "# grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "# filling in nan with mean of region\n",
    "grp_by_rs.fillna(grp_by_rs.groupby('region').transform('mean'), inplace=True)\n",
    "\n",
    "analyize_matches(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data2_te_dist[val_data2_te_dist.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data2_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data2[data2.index.isin(data2.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data2.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Exact matches:  0.3816793893129771\n",
      "Missed by 1:  0.4351145038167939\n",
      "Missed by 2:  0.1297709923664122\n",
      "Missed by 3:  0.05343511450381679\n",
      "Missed by 4:  0.0\n",
      "val rmse 1.1979626470874598\n",
      "trian rmse 0.8608408797388944\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data3.groupby(['region', 'season']).severity.expanding(1).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "print(grp_by_rs.isna().sum())\n",
    "# grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "# filling in nan with mean of region\n",
    "grp_by_rs.fillna(grp_by_rs.groupby('region').transform('mean'), inplace=True)\n",
    "\n",
    "analyize_matches(val_data3_te_dist[val_data3_te_dist.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data3_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data3_te_dist[val_data3_te_dist.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data3_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data3[data3.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data3.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Exact matches:  0.37404580152671757\n",
      "Missed by 1:  0.4351145038167939\n",
      "Missed by 2:  0.1297709923664122\n",
      "Missed by 3:  0.05343511450381679\n",
      "Missed by 4:  0.0\n",
      "val rmse 1.2097315482354263\n",
      "trian rmse 0.8618048550199326\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data3.groupby(['region', 'season']).severity.expanding(3).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "print(grp_by_rs.isna().sum())\n",
    "# grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "# filling in nan with mean of region\n",
    "grp_by_rs.fillna(grp_by_rs.groupby('region').transform('mean'), inplace=True)\n",
    "\n",
    "analyize_matches(val_data3_te_dist[val_data3_te_dist.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data3_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data3_te_dist[val_data3_te_dist.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data3_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data3[data3.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data3.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Exact matches:  0.37404580152671757\n",
      "Missed by 1:  0.4351145038167939\n",
      "Missed by 2:  0.1297709923664122\n",
      "Missed by 3:  0.05343511450381679\n",
      "Missed by 4:  0.0\n",
      "val rmse 1.209740613618887\n",
      "trian rmse 0.8618578815585727\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data3.groupby(['region', 'season']).severity.expanding(5).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "print(grp_by_rs.isna().sum())\n",
    "# grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "# filling in nan with mean of region\n",
    "grp_by_rs.fillna(grp_by_rs.groupby('region').transform('mean'), inplace=True)\n",
    "\n",
    "analyize_matches(val_data3_te_dist[val_data3_te_dist.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data3_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data3_te_dist[val_data3_te_dist.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data3_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data3[data3.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data3.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704\n",
      "Exact matches:  0.3435114503816794\n",
      "Missed by 1:  0.3969465648854962\n",
      "Missed by 2:  0.11450381679389313\n",
      "Missed by 3:  0.04580152671755725\n",
      "Missed by 4:  0.0\n",
      "val rmse 1.2065639638028516\n",
      "trian rmse 0.8622279194818819\n"
     ]
    }
   ],
   "source": [
    "grp_by_rs = data3.groupby(['region', 'season']).severity.expanding(50).mean()\n",
    "grp_by_rs = grp_by_rs.map(np.round)\n",
    "\n",
    "print(grp_by_rs.isna().sum())\n",
    "# grp_by_rs.fillna(reg_sev_map, inplace=True)\n",
    "# filling in nan with mean of region\n",
    "grp_by_rs.fillna(grp_by_rs.groupby('region').transform('mean'), inplace=True)\n",
    "\n",
    "analyize_matches(val_data3_te_dist[val_data3_te_dist.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data3_te_dist.index].sort_index())\n",
    "# rmse \n",
    "print(\"val rmse\", rmse(val_data3_te_dist[val_data3_te_dist.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[val_data3_te_dist.index].sort_index()))\n",
    "print(\"trian rmse\", rmse(data3[data3.index.isin(data3.index)].severity.sort_index(), grp_by_rs.droplevel(0).droplevel(0).loc[data3.index].sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if iam just like everyone else then how can i win everyone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aair</td>\n",
       "      <td>33.042600</td>\n",
       "      <td>-117.076000</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aajw</td>\n",
       "      <td>40.703968</td>\n",
       "      <td>-80.293050</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>aalr</td>\n",
       "      <td>38.972500</td>\n",
       "      <td>-94.672930</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>aalw</td>\n",
       "      <td>34.279000</td>\n",
       "      <td>-118.905000</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>zzpn</td>\n",
       "      <td>40.136410</td>\n",
       "      <td>-80.473740</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>zzrv</td>\n",
       "      <td>36.875400</td>\n",
       "      <td>-121.561000</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23563</th>\n",
       "      <td>zzsx</td>\n",
       "      <td>34.210000</td>\n",
       "      <td>-78.929389</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>south</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date split  year  month  week  \\\n",
       "1      aabn  36.559700 -121.510000 2016-08-31  test  2016      8    35   \n",
       "12     aair  33.042600 -117.076000 2014-11-01  test  2014     11    44   \n",
       "14     aajw  40.703968  -80.293050 2015-08-26  test  2015      8    35   \n",
       "15     aalr  38.972500  -94.672930 2019-08-26  test  2019      8    35   \n",
       "16     aalw  34.279000 -118.905000 2018-01-08  test  2018      1     2   \n",
       "...     ...        ...         ...        ...   ...   ...    ...   ...   \n",
       "23556  zzpn  40.136410  -80.473740 2019-07-08  test  2019      7    28   \n",
       "23560  zzrv  36.875400 -121.561000 2019-09-17  test  2019      9    38   \n",
       "23563  zzsx  34.210000  -78.929389 2019-07-16  test  2019      7    29   \n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02  test  2014     12    49   \n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31  test  2015      8    36   \n",
       "\n",
       "       season     region  severity  density  \n",
       "1           3       west       NaN      NaN  \n",
       "12          4       west       NaN      NaN  \n",
       "14          3  northeast       NaN      NaN  \n",
       "15          3    midwest       NaN      NaN  \n",
       "16          1       west       NaN      NaN  \n",
       "...       ...        ...       ...      ...  \n",
       "23556       3  northeast       NaN      NaN  \n",
       "23560       4       west       NaN      NaN  \n",
       "23563       3      south       NaN      NaN  \n",
       "23565       1       west       NaN      NaN  \n",
       "23569       3    midwest       NaN      NaN  \n",
       "\n",
       "[6510 rows x 12 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# k = 5\n",
    "\n",
    "# for row in tqdm(test_data[:100].itertuples(), total=test_data[:100].shape[0]):\n",
    "#     severity = knn(row, train_data=train_data, k=k)\n",
    "#     test_data.loc[test_data.uid == row.uid, f'nn{k}_guess'] = severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 50s\n",
      "Wall time: 13min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6510"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#  parrllelize the above code\n",
    "k = 20\n",
    "sev_list = Parallel(n_jobs=-1, backend='loky')([delayed(knn)(row, train_data=train_data, k=k) for row in test_data.itertuples()])\n",
    "len(sev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "      <th>latlng</th>\n",
       "      <th>nn10_guess</th>\n",
       "      <th>nn20_guess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.5597_-121.51</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aair</td>\n",
       "      <td>33.042600</td>\n",
       "      <td>-117.076000</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0426_-117.076</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aajw</td>\n",
       "      <td>40.703968</td>\n",
       "      <td>-80.293050</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.703968_-80.29305</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>aalr</td>\n",
       "      <td>38.972500</td>\n",
       "      <td>-94.672930</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.9725_-94.67293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>aalw</td>\n",
       "      <td>34.279000</td>\n",
       "      <td>-118.905000</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.279_-118.905</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>zzpn</td>\n",
       "      <td>40.136410</td>\n",
       "      <td>-80.473740</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.13641_-80.47374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>zzrv</td>\n",
       "      <td>36.875400</td>\n",
       "      <td>-121.561000</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.8754_-121.561</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23563</th>\n",
       "      <td>zzsx</td>\n",
       "      <td>34.210000</td>\n",
       "      <td>-78.929389</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>south</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.21_-78.9293894625038</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.7085_-121.749</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.767323_-96.028617</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date split  year  month  week  \\\n",
       "1      aabn  36.559700 -121.510000 2016-08-31  test  2016      8    35   \n",
       "12     aair  33.042600 -117.076000 2014-11-01  test  2014     11    44   \n",
       "14     aajw  40.703968  -80.293050 2015-08-26  test  2015      8    35   \n",
       "15     aalr  38.972500  -94.672930 2019-08-26  test  2019      8    35   \n",
       "16     aalw  34.279000 -118.905000 2018-01-08  test  2018      1     2   \n",
       "...     ...        ...         ...        ...   ...   ...    ...   ...   \n",
       "23556  zzpn  40.136410  -80.473740 2019-07-08  test  2019      7    28   \n",
       "23560  zzrv  36.875400 -121.561000 2019-09-17  test  2019      9    38   \n",
       "23563  zzsx  34.210000  -78.929389 2019-07-16  test  2019      7    29   \n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02  test  2014     12    49   \n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31  test  2015      8    36   \n",
       "\n",
       "       season     region  severity  density                   latlng  \\\n",
       "1           3       west       NaN      NaN          36.5597_-121.51   \n",
       "12          4       west       NaN      NaN         33.0426_-117.076   \n",
       "14          3  northeast       NaN      NaN      40.703968_-80.29305   \n",
       "15          3    midwest       NaN      NaN        38.9725_-94.67293   \n",
       "16          1       west       NaN      NaN          34.279_-118.905   \n",
       "...       ...        ...       ...      ...                      ...   \n",
       "23556       3  northeast       NaN      NaN       40.13641_-80.47374   \n",
       "23560       4       west       NaN      NaN         36.8754_-121.561   \n",
       "23563       3      south       NaN      NaN  34.21_-78.9293894625038   \n",
       "23565       1       west       NaN      NaN         36.7085_-121.749   \n",
       "23569       3    midwest       NaN      NaN     39.767323_-96.028617   \n",
       "\n",
       "       nn10_guess  nn20_guess  \n",
       "1             4.0         4.0  \n",
       "12            4.0         4.0  \n",
       "14            2.0         1.0  \n",
       "15            4.0         3.0  \n",
       "16            4.0         3.0  \n",
       "...           ...         ...  \n",
       "23556         1.0         2.0  \n",
       "23560         4.0         4.0  \n",
       "23563         1.0         1.0  \n",
       "23565         4.0         4.0  \n",
       "23569         3.0         3.0  \n",
       "\n",
       "[6510 rows x 15 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[f'nn{k}_guess'] = sev_list\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['latlng'] = train_data.latitude.astype(str) + \"_\" + train_data.longitude.astype(str)\n",
    "test_data['latlng'] = test_data.latitude.astype(str) + \"_\" + test_data.longitude.astype(str)\n",
    "\n",
    "len(set(test_data['latlng']).intersection(set(train_data['latlng'])))/len(set(test_data['latlng']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.358372\n",
       "4    0.301690\n",
       "1    0.230415\n",
       "3    0.109524\n",
       "Name: severity, dtype: float64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Making submission with knn-20\n",
    "\n",
    "assert sub_format.uid.equals(test_data.reset_index().uid) == True\n",
    "\n",
    "sub_format.severity = test_data.nn20_guess.values\n",
    "\n",
    "sub_format.severity = sub_format.severity.astype(int)\n",
    "sub_format.severity.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submission\n",
    "sub_format.to_csv('../submissions/to submit/nn20_guess_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  May be its better if start from "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sooo....\n",
    "\n",
    "Why am I behind?\n",
    "\n",
    "- I know for a fact that no one on the top is using images as features\n",
    "- How the train test split was done :\n",
    "    - No overlapping geo locations ( 0 % )\n",
    "    - only 51% of test date_regs are in train vs 92% in val\n",
    "    \n",
    "\n",
    "# Todos :\n",
    "\n",
    "- idk..\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "186db977f413ae6598820b658aed1651932768522d0652c07d949b3f5fb38b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
