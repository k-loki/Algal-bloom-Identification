{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Start modelling on metadata again. No imgs only metadata.``\n",
    "\n",
    "```find correlation with lb and reach 0.75s```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.distance import geodesic\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('../data/metadata.csv')\n",
    "sub_format = pd.read_csv('../data/submission_format.csv')\n",
    "train_labels = pd.read_csv('../data/train_labels.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mse(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dens_to_sev(x: float)-> int:\n",
    "    \"\"\"takes in density value in cells/ml and returns severity category\"\"\"\n",
    "    if (x < 20_000) : return 1\n",
    "    elif (x >= 20_000) and (x < 100_000) : return 2\n",
    "    elif (x >= 100_000) and (x < 1_000_000) : return 3\n",
    "    elif (x >= 1_000_000) and (x < 10_000_000) : return 4\n",
    "    elif x > 10_000_000 : return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_wise_peformance(y_data=None, preds=None):\n",
    "    \"\"\"\n",
    "    returns classifcation report for each region\n",
    "    y_data : pd.DataFrame\n",
    "    preds : np.array\n",
    "    \n",
    "    \"\"\"\n",
    "    y_data = y_data.copy(deep=True)\n",
    "    y_data['preds'] = preds\n",
    "    for region in sorted(y_data.region.unique()):\n",
    "        print(region)\n",
    "        print(classification_report(y_data[y_data.region == region].severity, y_data[y_data.region == region].preds))\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Utils\n",
    "def get_data_by_date( date=None, data=None):\n",
    "    return data[data.date == date]\n",
    "\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "\n",
    "def analyize_matches(y_true, y_pred, plot=False):\n",
    "    print(\"Exact matches: \", sum(y_true == y_pred) / len(y_true))\n",
    "    print(\"Missed by 1: \", sum(abs(y_true - y_pred) == 1) / len(y_true))\n",
    "    print(\"Missed by 2: \", sum(abs(y_true - y_pred) == 2) / len(y_true))\n",
    "    print(\"Missed by 3: \", sum(abs(y_true - y_pred) == 3) / len(y_true))\n",
    "    print(\"Missed by 4: \", sum(abs(y_true - y_pred) == 4) / len(y_true))\n",
    "    \n",
    "    stupid_vals = []\n",
    "    for i in range(1, 6):\n",
    "        stupid_vals.append(\n",
    "            ((sum([1 for x, y in zip(y_true, y_pred) if x == i and y == i])/len(y_true))*100, (sum(y_true == i)/len(y_true))*100)\n",
    "            )\n",
    "\n",
    "    print()\n",
    "    for i in range(5):\n",
    "        print(f\"Severity {i+1} : accuracy: {np.round(stupid_vals[i][0], 3)} % - prevalence: {np.round(stupid_vals[i][1], 3)} %\")\n",
    "\n",
    "    print()\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    if plot:\n",
    "        print()\n",
    "        sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add date fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabm</td>\n",
       "      <td>39.080319</td>\n",
       "      <td>-86.430867</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>train</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>585.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aacd</td>\n",
       "      <td>35.875083</td>\n",
       "      <td>-78.878434</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>train</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaee</td>\n",
       "      <td>35.487000</td>\n",
       "      <td>-79.062133</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>train</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaff</td>\n",
       "      <td>38.049471</td>\n",
       "      <td>-99.827001</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>train</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>3.0</td>\n",
       "      <td>111825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23566</th>\n",
       "      <td>zzwo</td>\n",
       "      <td>39.792190</td>\n",
       "      <td>-99.971050</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>train</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23567</th>\n",
       "      <td>zzwq</td>\n",
       "      <td>35.794000</td>\n",
       "      <td>-79.012551</td>\n",
       "      <td>2015-03-24</td>\n",
       "      <td>train</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23568</th>\n",
       "      <td>zzyb</td>\n",
       "      <td>35.742000</td>\n",
       "      <td>-79.238600</td>\n",
       "      <td>2016-11-21</td>\n",
       "      <td>train</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>south</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23570 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date  split  year  month  week  \\\n",
       "0      aabm  39.080319  -86.430867 2018-05-14  train  2018      5    20   \n",
       "1      aabn  36.559700 -121.510000 2016-08-31   test  2016      8    35   \n",
       "2      aacd  35.875083  -78.878434 2020-11-19  train  2020     11    47   \n",
       "3      aaee  35.487000  -79.062133 2016-08-24  train  2016      8    34   \n",
       "4      aaff  38.049471  -99.827001 2019-07-23  train  2019      7    30   \n",
       "...     ...        ...         ...        ...    ...   ...    ...   ...   \n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02   test  2014     12    49   \n",
       "23566  zzwo  39.792190  -99.971050 2017-06-19  train  2017      6    25   \n",
       "23567  zzwq  35.794000  -79.012551 2015-03-24  train  2015      3    13   \n",
       "23568  zzyb  35.742000  -79.238600 2016-11-21  train  2016     11    47   \n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31   test  2015      8    36   \n",
       "\n",
       "       season   region  severity   density  \n",
       "0           2  midwest       1.0     585.0  \n",
       "1           3     west       NaN       NaN  \n",
       "2           4    south       1.0     290.0  \n",
       "3           3    south       1.0    1614.0  \n",
       "4           3  midwest       3.0  111825.0  \n",
       "...       ...      ...       ...       ...  \n",
       "23565       1     west       NaN       NaN  \n",
       "23566       3  midwest       2.0   48510.0  \n",
       "23567       2    south       1.0    1271.0  \n",
       "23568       4    south       1.0    9682.0  \n",
       "23569       3  midwest       NaN       NaN  \n",
       "\n",
       "[23570 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.date = pd.to_datetime(metadata.date)\n",
    "metadata['year'] = metadata.date.dt.year\n",
    "metadata['month'] = metadata.date.dt.month\n",
    "metadata['week'] = metadata.date.dt.isocalendar().week\n",
    "\n",
    "seasons = {\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "    6: 3,\n",
    "    7: 3,\n",
    "    8: 3,\n",
    "    9: 4,\n",
    "    10: 4,\n",
    "    11: 4,\n",
    "    12: 1\n",
    "}\n",
    "\n",
    "reg_sev_map = {\n",
    "    'midwest': 2,\n",
    "    'northeast': 2,\n",
    "    'south' : 2,\n",
    "    'west' : 4\n",
    "}\n",
    "\n",
    "reg_map = {\n",
    "    'south' : 0,\n",
    "    'northeast' : 1,\n",
    "    'west' : 2,\n",
    "    'midwest' : 3\n",
    "}\n",
    "\n",
    "metadata['season'] = metadata.month.map(seasons)\n",
    "\n",
    "region = pd.concat((train_labels, sub_format[['region', 'uid']]), axis=0)\n",
    "\n",
    "data = pd.merge(metadata, region, on='uid', how='left')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6510, 12), (23570, 12))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = data[data.split == 'test']\n",
    "test_data.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17060, 12), (23570, 12))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = data[data.split == 'train']\n",
    "train_data.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14501, 12), (2559, 12))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data, val_data = train_test_split(train_data, test_size=0.15, random_state=123456789, shuffle=True)\n",
    "tr_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      " % of intersection between date and regions in val and train sets before correction: 0.9328793774319066\n",
      " % of intersection between date and regions in val and train sets after correction: 0.36867704280155644\n",
      " % of intersection between dates in val and train sets: 0.656211                                                                    \n",
      " % of intersection between date in test and train sets: 0.701095                                                                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5617, 13), (2559, 13), (17060, 12), (2559, 13))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data similar to test data\n",
    "val_data['date_reg'] = val_data.date.astype(str) + \"_\" +  val_data.region\n",
    "tr_data['date_reg'] = tr_data.date.astype(str) + \"_\" +  tr_data.region\n",
    "\n",
    "assert (val_data.columns == tr_data.columns).all()\n",
    "\n",
    "print(set(val_data.uid).intersection(set(tr_data.uid)))\n",
    "\n",
    "# percentage of intersection date and regs before\n",
    "print(f\" % of intersection between date and regions in val and train sets before correction: {len(set(val_data.date_reg).intersection(set(tr_data.date_reg)))/val_data.date_reg.nunique()}\")\n",
    "\n",
    "datereg_to_remove = val_data.date_reg.sample(frac=0.40, random_state=123456789)\n",
    "tr_data2_te_dist = tr_data[~tr_data.date_reg.isin(datereg_to_remove)]\n",
    "val_data2_te_dist = val_data\n",
    "\n",
    "print(f\" % of intersection between date and regions in val and train sets after correction: {len(set(val_data2_te_dist.date_reg).intersection(set(tr_data2_te_dist.date_reg)))/val_data2_te_dist.date_reg.nunique()}\")\n",
    "\n",
    "\n",
    "print(f\" % of intersection between dates in val and train sets: {len(set(val_data2_te_dist.date).intersection(set(tr_data2_te_dist.date)))/val_data2_te_dist.date.nunique() :<75f} \")\n",
    "print(f\" % of intersection between date in test and train sets: {len(set(test_data.date).intersection(set(train_data.date)))/test_data.date.nunique():<75f}\" )\n",
    "\n",
    "tr_data2_te_dist.shape, val_data2_te_dist.shape, train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching dateregs btw val and tr: 0.4\n",
      "matching dates btw val and tr: 0.7589285714285714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14501, 13), (131, 13))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data3 = tr_data.copy()\n",
    "val_data3 = val_data.copy()\n",
    "\n",
    "datereg_to_remove = tr_data3.date_reg.sample(frac=0.60, random_state=123456789)\n",
    "val_data3_te_dist = val_data3[~val_data3.date_reg.isin(datereg_to_remove)]\n",
    "tr_data3_te_dist = tr_data3\n",
    "\n",
    "print(\"matching dateregs btw val and tr:\", len(set(val_data3_te_dist.date_reg).intersection(set(tr_data3_te_dist.date_reg)))/val_data3_te_dist.date_reg.nunique())\n",
    "print(\"matching dates btw val and tr:\",len(set(val_data3_te_dist.date).intersection(set(tr_data3_te_dist.date)))/val_data3_te_dist.date.nunique())\n",
    "\n",
    "tr_data3_te_dist.shape, val_data3_te_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11968, 12), (5092, 12))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  split by time\n",
    "train_data = train_data.sort_values(by='date')\n",
    "train_data_ts = train_data[train_data.date < '2019-01-01']\n",
    "val_data_ts = train_data[train_data.date >= '2019-01-01']\n",
    "\n",
    "train_data_ts.shape, val_data_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data.sort_values(by='date', inplace=True)\n",
    "val_data.sort_values(by='date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aair</td>\n",
       "      <td>33.042600</td>\n",
       "      <td>-117.076000</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aajw</td>\n",
       "      <td>40.703968</td>\n",
       "      <td>-80.293050</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>aalr</td>\n",
       "      <td>38.972500</td>\n",
       "      <td>-94.672930</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>aalw</td>\n",
       "      <td>34.279000</td>\n",
       "      <td>-118.905000</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>zzpn</td>\n",
       "      <td>40.136410</td>\n",
       "      <td>-80.473740</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>zzrv</td>\n",
       "      <td>36.875400</td>\n",
       "      <td>-121.561000</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23563</th>\n",
       "      <td>zzsx</td>\n",
       "      <td>34.210000</td>\n",
       "      <td>-78.929389</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>south</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date split  year  month  week  \\\n",
       "1      aabn  36.559700 -121.510000 2016-08-31  test  2016      8    35   \n",
       "12     aair  33.042600 -117.076000 2014-11-01  test  2014     11    44   \n",
       "14     aajw  40.703968  -80.293050 2015-08-26  test  2015      8    35   \n",
       "15     aalr  38.972500  -94.672930 2019-08-26  test  2019      8    35   \n",
       "16     aalw  34.279000 -118.905000 2018-01-08  test  2018      1     2   \n",
       "...     ...        ...         ...        ...   ...   ...    ...   ...   \n",
       "23556  zzpn  40.136410  -80.473740 2019-07-08  test  2019      7    28   \n",
       "23560  zzrv  36.875400 -121.561000 2019-09-17  test  2019      9    38   \n",
       "23563  zzsx  34.210000  -78.929389 2019-07-16  test  2019      7    29   \n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02  test  2014     12    49   \n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31  test  2015      8    36   \n",
       "\n",
       "       season     region  severity  density  \n",
       "1           3       west       NaN      NaN  \n",
       "12          4       west       NaN      NaN  \n",
       "14          3  northeast       NaN      NaN  \n",
       "15          3    midwest       NaN      NaN  \n",
       "16          1       west       NaN      NaN  \n",
       "...       ...        ...       ...      ...  \n",
       "23556       3  northeast       NaN      NaN  \n",
       "23560       4       west       NaN      NaN  \n",
       "23563       3      south       NaN      NaN  \n",
       "23565       1       west       NaN      NaN  \n",
       "23569       3    midwest       NaN      NaN  \n",
       "\n",
       "[6510 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aair</td>\n",
       "      <td>33.042600</td>\n",
       "      <td>-117.076000</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aajw</td>\n",
       "      <td>40.703968</td>\n",
       "      <td>-80.293050</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>aalr</td>\n",
       "      <td>38.972500</td>\n",
       "      <td>-94.672930</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>aalw</td>\n",
       "      <td>34.279000</td>\n",
       "      <td>-118.905000</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>zzpn</td>\n",
       "      <td>40.136410</td>\n",
       "      <td>-80.473740</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>zzrv</td>\n",
       "      <td>36.875400</td>\n",
       "      <td>-121.561000</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23563</th>\n",
       "      <td>zzsx</td>\n",
       "      <td>34.210000</td>\n",
       "      <td>-78.929389</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>south</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date split  year  month  week  \\\n",
       "1      aabn  36.559700 -121.510000 2016-08-31  test  2016      8    35   \n",
       "12     aair  33.042600 -117.076000 2014-11-01  test  2014     11    44   \n",
       "14     aajw  40.703968  -80.293050 2015-08-26  test  2015      8    35   \n",
       "15     aalr  38.972500  -94.672930 2019-08-26  test  2019      8    35   \n",
       "16     aalw  34.279000 -118.905000 2018-01-08  test  2018      1     2   \n",
       "...     ...        ...         ...        ...   ...   ...    ...   ...   \n",
       "23556  zzpn  40.136410  -80.473740 2019-07-08  test  2019      7    28   \n",
       "23560  zzrv  36.875400 -121.561000 2019-09-17  test  2019      9    38   \n",
       "23563  zzsx  34.210000  -78.929389 2019-07-16  test  2019      7    29   \n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02  test  2014     12    49   \n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31  test  2015      8    36   \n",
       "\n",
       "       season     region  severity  density  \n",
       "1           3       west       NaN      NaN  \n",
       "12          4       west       NaN      NaN  \n",
       "14          3  northeast       NaN      NaN  \n",
       "15          3    midwest       NaN      NaN  \n",
       "16          1       west       NaN      NaN  \n",
       "...       ...        ...       ...      ...  \n",
       "23556       3  northeast       NaN      NaN  \n",
       "23560       4       west       NaN      NaN  \n",
       "23563       3      south       NaN      NaN  \n",
       "23565       1       west       NaN      NaN  \n",
       "23569       3    midwest       NaN      NaN  \n",
       "\n",
       "[6510 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_data[train_data.columns]\n",
    "X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>split</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabn</td>\n",
       "      <td>36.559700</td>\n",
       "      <td>-121.510000</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aair</td>\n",
       "      <td>33.042600</td>\n",
       "      <td>-117.076000</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aajw</td>\n",
       "      <td>40.703968</td>\n",
       "      <td>-80.293050</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>aalr</td>\n",
       "      <td>38.972500</td>\n",
       "      <td>-94.672930</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>aalw</td>\n",
       "      <td>34.279000</td>\n",
       "      <td>-118.905000</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>zzpn</td>\n",
       "      <td>40.136410</td>\n",
       "      <td>-80.473740</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>northeast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>zzrv</td>\n",
       "      <td>36.875400</td>\n",
       "      <td>-121.561000</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23563</th>\n",
       "      <td>zzsx</td>\n",
       "      <td>34.210000</td>\n",
       "      <td>-78.929389</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>test</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>south</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>36.708500</td>\n",
       "      <td>-121.749000</td>\n",
       "      <td>2014-12-02</td>\n",
       "      <td>test</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>west</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>39.767323</td>\n",
       "      <td>-96.028617</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>test</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>midwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   latitude   longitude       date split  year  month  week  \\\n",
       "1      aabn  36.559700 -121.510000 2016-08-31  test  2016      8    35   \n",
       "12     aair  33.042600 -117.076000 2014-11-01  test  2014     11    44   \n",
       "14     aajw  40.703968  -80.293050 2015-08-26  test  2015      8    35   \n",
       "15     aalr  38.972500  -94.672930 2019-08-26  test  2019      8    35   \n",
       "16     aalw  34.279000 -118.905000 2018-01-08  test  2018      1     2   \n",
       "...     ...        ...         ...        ...   ...   ...    ...   ...   \n",
       "23556  zzpn  40.136410  -80.473740 2019-07-08  test  2019      7    28   \n",
       "23560  zzrv  36.875400 -121.561000 2019-09-17  test  2019      9    38   \n",
       "23563  zzsx  34.210000  -78.929389 2019-07-16  test  2019      7    29   \n",
       "23565  zzvv  36.708500 -121.749000 2014-12-02  test  2014     12    49   \n",
       "23569  zzzi  39.767323  -96.028617 2015-08-31  test  2015      8    36   \n",
       "\n",
       "       season     region  severity  density  \n",
       "1           3       west       NaN      NaN  \n",
       "12          4       west       NaN      NaN  \n",
       "14          3  northeast       NaN      NaN  \n",
       "15          3    midwest       NaN      NaN  \n",
       "16          1       west       NaN      NaN  \n",
       "...       ...        ...       ...      ...  \n",
       "23556       3  northeast       NaN      NaN  \n",
       "23560       4       west       NaN      NaN  \n",
       "23563       3      south       NaN      NaN  \n",
       "23565       1       west       NaN      NaN  \n",
       "23569       3    midwest       NaN      NaN  \n",
       "\n",
       "[6510 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.36377549791398, -95.87638595051324)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.latitude.mean(), test_data.longitude.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.59305327688648"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_distance(38.36377549791398,-95.87638595051324 ,38.36377549791398,-95.0000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearly the test data will have slightly more avg severity "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, X_train=None, X_val=None, y_train=None, y_val=None, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    train and eval util func,\n",
    "    returns trained model, soft_preds, and tr, val, test rmses\n",
    "    REMEMBER returns soft-preds\n",
    "    \"\"\"\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    val_preds = model.predict(X_val)\n",
    "    val_rmse = rmse(y_val, np.clip(np.round(val_preds), 1, 5))\n",
    "    train_rmse = rmse(y_train, np.clip(np.round(model.predict(X_train)), 1, 5))   # don't forget to clip!\n",
    "    print(\"Train RMSE: \", train_rmse)\n",
    "    print(\"Val RMSE:\", val_rmse)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_rmse = mse(y_test, np.clip(np.round(model.predict(X_test)), 1, 5), squared=False)\n",
    "        print('TEST RMSE: ', test_rmse)\n",
    "    else:\n",
    "        test_rmse = 0\n",
    "        \n",
    "    # print(\"TEST RMSE:\", mse(y_val, np.round(model.predict(X_val)), squared=False))\n",
    "    return model, val_preds, train_rmse, val_rmse, test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_it(model, X_train=None, y_train=None, X_test=None, y_test=None, splits=10, cv_predict=False):\n",
    "    \"\"\" \n",
    "    cv in stratified kfold fashion, \n",
    "    returns train, val, test rmses, and cv preds for test and train if cv_predict=True\n",
    "    remember to round preds\n",
    "    And also send in \"dates\" to make val set similar to test set\n",
    "    \"\"\"\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=splits, random_state=12_000, shuffle=True)\n",
    "    # tscv = TimeSeriesSplit(n_splits=splits, test_size=200)\n",
    "    print('Using cols: ', X_train.columns.values)\n",
    "\n",
    "    train_rmses = []\n",
    "    val_rmses = []\n",
    "    test_rmses = []\n",
    "    cvpreds_test, cvpreds_train = None, None\n",
    "\n",
    "    if cv_predict:\n",
    "        if X_test is not None: test_shape = X_test.shape[0]\n",
    "        else: test_shape = 6510\n",
    "        cvpreds_test = np.zeros(shape=(test_shape, splits))\n",
    "        cvpreds_train = np.zeros(shape=(len(X_train)))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "\n",
    "        print(f'----------------------------------Fold-{fold}------------------------------------')\n",
    "        X_train_subset, y_train_subset = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "        X_val_subset, y_val_subset = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        try:\n",
    "            # make val set more similar to test set by making val_set.latlng.isin(trian_set.latlng) --> 0\n",
    "            X_val_subset['latlng'] = X_val_subset.latitude.astype(str) + '_' + X_val_subset.longitude.astype(str)\n",
    "            X_train_subset['latlng'] = X_train_subset.latitude.astype(str) + '_' + X_train_subset.longitude.astype(str)\n",
    "\n",
    "            drp_idx = X_val_subset[X_val_subset.latlng.isin(X_train_subset.latlng)].index\n",
    "            X_val_subset.drop(drp_idx, inplace=True)\n",
    "            y_val_subset.drop(drp_idx, inplace=True)\n",
    "\n",
    "            assert X_val_subset.latlng.isin(X_train_subset.latlng).sum() == 0\n",
    "            \n",
    "            # drop latlng\n",
    "            X_val_subset.drop('latlng', axis=1, inplace=True)\n",
    "            X_train_subset.drop('latlng', axis=1, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        assert X_train_subset.columns.tolist() == X_val_subset.columns.tolist()\n",
    "        assert X_train_subset.columns.tolist() == X_test.columns.tolist()\n",
    "\n",
    "        print(f'Training on {X_train_subset.shape[0]} samples' )\n",
    "        print(f'Validating on {X_val_subset.shape[0]} samples' )\n",
    "\n",
    "        print(y_val_subset.value_counts(normalize=True))\n",
    "        model, val_preds, train_rmse, val_rmse, test_rmse= train_eval(\n",
    "                                    model=model, \n",
    "                                    X_train=X_train_subset, \n",
    "                                    y_train=y_train_subset, \n",
    "                                    X_val=X_val_subset, \n",
    "                                    y_val=y_val_subset,\n",
    "                                    X_test=X_test,\n",
    "                                    y_test=y_test\n",
    "                                    )\n",
    "\n",
    "        sns.barplot(model.feature_importances_, model.feature_names_in_)\n",
    "\n",
    "        val_rmses.append(val_rmse)\n",
    "        train_rmses.append(train_rmse)\n",
    "        test_rmses.append(test_rmse)\n",
    "        \n",
    "        if cv_predict:\n",
    "            # save predictions for ensembling\n",
    "            if X_test is not None:\n",
    "                cvpreds_test[:, fold] = model.predict(X_test)\n",
    "            cvpreds_train[val_idx] = model.predict(X_train.iloc[val_idx])\n",
    "                    \n",
    "    print()\n",
    "    print(\"Mean Train RMSE:\", np.mean(train_rmses), \"std:\", np.std(train_rmses))\n",
    "    print(\"Mean Val RMSE:\", np.mean(val_rmses), \"std:\", np.std(val_rmses))\n",
    "    print(\"Mean Test RMSE:\", np.mean(test_rmses), \"std:\", np.std(test_rmses))\n",
    "    \n",
    "    return cvpreds_test, cvpreds_train, np.mean(train_rmses), np.mean(val_rmses), np.mean(test_rmses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(row=None, train_data=tr_data, k=1):\n",
    "    \"\"\"\n",
    "    row : pd.Series (row from val_data)\n",
    "    train_data : pd.DataFrame (all_data)\n",
    "    k : int number of nearest neighbours to consider\n",
    "    \n",
    "    algo:\n",
    "    1. Get past month data collected till the current row\n",
    "    2. Get the k nearest neighbours (geodesic dist using lat, lng) from the above data\n",
    "    3. Get the mean of the severity from the above rows\n",
    "    4. Return the mean of the nearest neighbours severity\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if row is None:\n",
    "        print('Row None bruv!')\n",
    "        return None\n",
    "    \n",
    "    uid = row.uid\n",
    "    date = row.date\n",
    "    region = row.region\n",
    "    past_date = date - pd.Timedelta(days=30)\n",
    "    \n",
    "    past_month_data = train_data[(train_data.date < date) & (train_data.date >= past_date)]\n",
    "    past_month_data.sort_values(by='date', inplace=True)\n",
    "    \n",
    "    # if no past data, return the mean of the region\n",
    "    if len(past_month_data) == 0:\n",
    "        return reg_sev_map[region]\n",
    "        \n",
    "    dist_matrix =pd.DataFrame(columns=['uid', 'dist'])       # 0th col for uid, 1st col for dist\n",
    "    for i, past_row in enumerate(past_month_data.itertuples()):\n",
    "        dist_matrix.loc[i, 'uid'] = past_row.uid\n",
    "        dist_matrix.loc[i, 'dist'] = get_distance(row.latitude, row.longitude, past_row.latitude, past_row.longitude)   # returns geodesic dist in km\n",
    "\n",
    "    # get mean of top k nearest neighbours\n",
    "    n_uids = dist_matrix.sort_values(by='dist').head(k).uid.values\n",
    "    nn_severity = train_data[train_data.uid.isin(n_uids)].severity.mean()\n",
    "    \n",
    "    return np.round(nn_severity)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'latitude', 'longitude', 'date', 'split', 'year', 'month',\n",
       "       'week', 'season', 'region', 'severity', 'density', 'date_reg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['date', 'uid', 'severity', 'year', 'split', 'density', 'date_reg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = tr_data.drop(columns=drop_cols)\n",
    "X_train.week = X_train.week.astype('int')\n",
    "X_train.region = X_train.region.map(reg_map)  # reg map is better than reg_sev_map\n",
    "y_train = tr_data.severity\n",
    "\n",
    "X_val = val_data.drop(columns=drop_cols)\n",
    "X_val.week = X_val.week.astype('int')\n",
    "X_val.region = X_val.region.map(reg_map)\n",
    "y_val = val_data.severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = XGBRegressor(n_estimators=1000, n_jobs=-1, learning_rate=0.1, max_depth=5, random_state=12_000, gpu_id=0, tree_method='gpu_hist')\n",
    "# model, _, *rmses = train_eval(model=xgb, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "# sns.barplot(model.feature_names_in_, model.feature_importances_)\n",
    "# plt.title('with depth 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  treees are soo fucking unstable...\n",
    "# trees trained on gpu are different from trees trained on cpu\n",
    "# probabaly tuning and ensembling will help and add. fts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.columns.to_list() == X_val.columns.to_list()\n",
    "assert y_train.index.to_list() == X_train.index.to_list()\n",
    "assert y_val.index.to_list() == X_val.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17448</th>\n",
       "      <td>38.78500</td>\n",
       "      <td>-121.653000</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12985</th>\n",
       "      <td>39.21155</td>\n",
       "      <td>-97.005590</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21075</th>\n",
       "      <td>36.10000</td>\n",
       "      <td>-78.896082</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       latitude   longitude  month  week  season  region\n",
       "17448  38.78500 -121.653000      6    25       3       2\n",
       "12985  39.21155  -97.005590      7    29       3       3\n",
       "21075  36.10000  -78.896082      8    35       3       0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data[['latitude', 'longitude']] = test_data[['latitude', 'longitude']].apply(np.round)\n",
    "# train_data[['latitude', 'longitude']] = train_data[['latitude', 'longitude']].apply(np.round)\n",
    "\n",
    "# tr_data[['latitude', 'longitude']] = tr_data[['latitude', 'longitude']].apply(np.round)\n",
    "# val_data[['latitude', 'longitude']] = val_data[['latitude', 'longitude']].apply(np.round)\n",
    "\n",
    "\n",
    "# test_data['latlng'] = test_data.latitude.astype(str) + \"_\" + test_data.longitude.astype(str)\n",
    "# train_data['latlng'] = train_data.latitude.astype(str) + \"_\" + train_data.longitude.astype(str)\n",
    "\n",
    "# tr_data['latlng'] = tr_data.latitude.astype(str) + \"_\" + tr_data.longitude.astype(str)\n",
    "# val_data['latlng'] = val_data.latitude.astype(str) + \"_\" + val_data.longitude.astype(str)\n",
    "\n",
    "\n",
    "# # % of testdata latlng in train data\n",
    "# print(f\" % of testdata latlng in train data: {len(set(test_data.latlng).intersection(set(train_data.latlng)))/test_data.latlng.nunique():<75f}\")\n",
    "\n",
    "# print(f\" % of val_data latlng in tr data: {len(set(val_data.latlng).intersection(set(tr_data.latlng)))/val_data.latlng.nunique():<75f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with rounded lat and long,(approx. 90 km diff) rmse @ 1.04 +- 0.16\n",
    "# so the trees are creating splits on lat and long for every coord in train data so they are failing on test data! (which have abs 0 lat and long from test data.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.0803190688889"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.latitude[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14501,), (14501,), (2559,), (2559,), (6510,), (6510,))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lats = X_train.latitude\n",
    "train_lngs = X_train.longitude\n",
    "\n",
    "val_lats = X_val.latitude\n",
    "val_lngs = X_val.longitude\n",
    "\n",
    "test_lats = test_data.latitude\n",
    "test_lngs = test_data.longitude\n",
    "\n",
    "train_lats.shape, train_lngs.shape, val_lats.shape, val_lngs.shape, test_lats.shape, test_lngs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_rmses = pd.DataFrame(columns=['train_rmse', 'val_rmse', 'test_rmse'], index=range(0, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 0 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 15 samples\n",
      "2.0    0.533333\n",
      "3.0    0.200000\n",
      "4.0    0.133333\n",
      "1.0    0.133333\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.7164728420068226\n",
      "Val RMSE: 1.1547005383792515\n",
      "TEST RMSE:  0.7782707457265569\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 12 samples\n",
      "2.0    0.583333\n",
      "3.0    0.250000\n",
      "4.0    0.166667\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.7156428321755175\n",
      "Val RMSE: 1.0\n",
      "TEST RMSE:  0.7762597068466343\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 12 samples\n",
      "2.0    0.416667\n",
      "3.0    0.333333\n",
      "1.0    0.166667\n",
      "4.0    0.083333\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.718154523659696\n",
      "Val RMSE: 1.0\n",
      "TEST RMSE:  0.7785217603261257\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 30 samples\n",
      "3.0    0.466667\n",
      "2.0    0.266667\n",
      "1.0    0.133333\n",
      "4.0    0.100000\n",
      "5.0    0.033333\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.7183145462093836\n",
      "Val RMSE: 1.2382783747337807\n",
      "TEST RMSE:  0.7820275048657492\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 11 samples\n",
      "2.0    0.363636\n",
      "1.0    0.272727\n",
      "3.0    0.272727\n",
      "4.0    0.090909\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.7196999178844798\n",
      "Val RMSE: 1.2792042981336627\n",
      "TEST RMSE:  0.7775172157008585\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 16 samples\n",
      "3.0    0.5000\n",
      "1.0    0.2500\n",
      "2.0    0.1875\n",
      "4.0    0.0625\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.7178343715417013\n",
      "Val RMSE: 0.9354143466934853\n",
      "TEST RMSE:  0.7752522311374204\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 22 samples\n",
      "2.0    0.363636\n",
      "1.0    0.227273\n",
      "3.0    0.227273\n",
      "4.0    0.181818\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.718687793595553\n",
      "Val RMSE: 1.2613124477737825\n",
      "TEST RMSE:  0.7765113715336956\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 11 samples\n",
      "3.0    0.545455\n",
      "2.0    0.272727\n",
      "1.0    0.181818\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.7163919164666076\n",
      "Val RMSE: 0.9045340337332909\n",
      "TEST RMSE:  0.7792743189941814\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 22 samples\n",
      "3.0    0.454545\n",
      "2.0    0.409091\n",
      "4.0    0.090909\n",
      "1.0    0.045455\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.7142495913455335\n",
      "Val RMSE: 0.9534625892455924\n",
      "TEST RMSE:  0.7775172157008585\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 14 samples\n",
      "3.0    0.428571\n",
      "2.0    0.357143\n",
      "1.0    0.214286\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.7170867940717959\n",
      "Val RMSE: 0.7559289460184544\n",
      "TEST RMSE:  0.777768473492423\n",
      "\n",
      "Mean Train RMSE: 0.7172535128957092 std: 0.0015269084786865032\n",
      "Mean Val RMSE: 1.0482835574711298 std: 0.1669300823690604\n",
      "Mean Test RMSE: 0.7778920544324504 std: 0.0017716752961140535\n",
      "Exact matches:  0.5463071512309496\n",
      "Missed by 1:  0.4126611957796014\n",
      "Missed by 2:  0.03751465416178194\n",
      "Missed by 3:  0.0031262211801484957\n",
      "Missed by 4:  0.00039077764751856197\n",
      "\n",
      "Severity 1 : accuracy: 19.93 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 12.661 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 3.322 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.718 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.45      0.57      1124\n",
      "         2.0       0.27      0.66      0.39       492\n",
      "         3.0       0.46      0.21      0.29       405\n",
      "         4.0       0.91      0.91      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.55      2559\n",
      "   macro avg       0.48      0.45      0.43      2559\n",
      "weighted avg       0.65      0.55      0.56      2559\n",
      "\n",
      "Exact matches:  0.547962209502793\n",
      "Missed by 1:  0.4113509413143921\n",
      "Missed by 2:  0.03813530101372319\n",
      "Missed by 3:  0.0022067443624577614\n",
      "Missed by 4:  0.0003448038066340252\n",
      "\n",
      "Severity 1 : accuracy: 19.64 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 12.468 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 3.882 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.792 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.014 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.75      0.45      0.56      6373\n",
      "         2.0       0.27      0.66      0.39      2747\n",
      "         3.0       0.51      0.24      0.33      2314\n",
      "         4.0       0.92      0.90      0.91      3020\n",
      "         5.0       0.40      0.04      0.08        47\n",
      "\n",
      "    accuracy                           0.55     14501\n",
      "   macro avg       0.57      0.46      0.45     14501\n",
      "weighted avg       0.65      0.55      0.56     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.93      0.66      0.77       665\n",
      "         2.0       0.32      0.50      0.39       362\n",
      "         3.0       0.54      0.65      0.59       658\n",
      "         4.0       0.60      0.15      0.24       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.58      1860\n",
      "   macro avg       0.48      0.39      0.40      1860\n",
      "weighted avg       0.64      0.58      0.58      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.56      0.64       472\n",
      "         2.0       0.40      0.64      0.49       278\n",
      "         3.0       0.49      0.48      0.48       182\n",
      "         4.0       0.56      0.12      0.20        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.54       984\n",
      "   macro avg       0.44      0.36      0.36       984\n",
      "weighted avg       0.59      0.54      0.54       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.40      0.52      5102\n",
      "         2.0       0.26      0.70      0.38      2022\n",
      "         3.0       0.44      0.02      0.04      1229\n",
      "         4.0       0.50      0.03      0.06        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.41      8457\n",
      "   macro avg       0.38      0.23      0.20      8457\n",
      "weighted avg       0.56      0.41      0.41      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.75      0.67      0.71       134\n",
      "         2.0       0.38      0.36      0.37        85\n",
      "         3.0       0.29      0.09      0.13       245\n",
      "         4.0       0.92      0.99      0.96      2716\n",
      "         5.0       0.50      0.10      0.17        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.57      0.44      0.47      3200\n",
      "weighted avg       0.85      0.89      0.86      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.98      0.69      0.81       116\n",
      "         2.0       0.35      0.54      0.43        76\n",
      "         3.0       0.51      0.62      0.56       113\n",
      "         4.0       0.50      0.10      0.17        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.57       340\n",
      "   macro avg       0.47      0.39      0.39       340\n",
      "weighted avg       0.63      0.57      0.57       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.46      0.57        84\n",
      "         2.0       0.35      0.65      0.46        43\n",
      "         3.0       0.33      0.29      0.31        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.47       159\n",
      "   macro avg       0.28      0.28      0.27       159\n",
      "weighted avg       0.54      0.47      0.48       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.73      0.41      0.53       897\n",
      "         2.0       0.26      0.70      0.38       356\n",
      "         3.0       0.25      0.01      0.02       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.42      1491\n",
      "   macro avg       0.25      0.23      0.18      1491\n",
      "weighted avg       0.54      0.42      0.41      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.70      0.75        27\n",
      "         2.0       0.45      0.29      0.36        17\n",
      "         3.0       0.31      0.12      0.17        43\n",
      "         4.0       0.92      0.99      0.95       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.50      0.42      0.45       569\n",
      "weighted avg       0.85      0.89      0.86       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 1 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 39 samples\n",
      "2.0    0.384615\n",
      "1.0    0.256410\n",
      "3.0    0.230769\n",
      "4.0    0.128205\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6265517135946793\n",
      "Val RMSE: 1.1435437497937313\n",
      "TEST RMSE:  0.7316866659304582\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 32 samples\n",
      "2.0    0.50000\n",
      "3.0    0.31250\n",
      "4.0    0.09375\n",
      "1.0    0.09375\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6279325500489096\n",
      "Val RMSE: 0.8100925873009825\n",
      "TEST RMSE:  0.7356813452937407\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 48 samples\n",
      "1.0    0.291667\n",
      "2.0    0.291667\n",
      "3.0    0.291667\n",
      "4.0    0.125000\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6287861338686067\n",
      "Val RMSE: 0.8779711460710616\n",
      "TEST RMSE:  0.7298149995497489\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 71 samples\n",
      "3.0    0.408451\n",
      "2.0    0.239437\n",
      "1.0    0.197183\n",
      "4.0    0.126761\n",
      "5.0    0.028169\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6333998032081796\n",
      "Val RMSE: 1.1196075771271663\n",
      "TEST RMSE:  0.7282068852206003\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 53 samples\n",
      "3.0    0.377358\n",
      "2.0    0.283019\n",
      "1.0    0.207547\n",
      "4.0    0.113208\n",
      "5.0    0.018868\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6346083519186296\n",
      "Val RMSE: 1.0815781621106804\n",
      "TEST RMSE:  0.7319536561476747\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 44 samples\n",
      "3.0    0.431818\n",
      "2.0    0.250000\n",
      "1.0    0.181818\n",
      "4.0    0.136364\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6318252339035109\n",
      "Val RMSE: 1.0335288182638247\n",
      "TEST RMSE:  0.7340860820967096\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 50 samples\n",
      "3.0    0.34\n",
      "2.0    0.32\n",
      "4.0    0.18\n",
      "1.0    0.14\n",
      "5.0    0.02\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6311578874622664\n",
      "Val RMSE: 1.104536101718726\n",
      "TEST RMSE:  0.7338198677336137\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 47 samples\n",
      "3.0    0.382979\n",
      "2.0    0.297872\n",
      "4.0    0.170213\n",
      "1.0    0.148936\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6311578874622664\n",
      "Val RMSE: 1.0718844015157447\n",
      "TEST RMSE:  0.7330206445510403\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 62 samples\n",
      "2.0    0.370968\n",
      "3.0    0.354839\n",
      "1.0    0.225806\n",
      "4.0    0.048387\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6300035331378294\n",
      "Val RMSE: 0.9069623173877128\n",
      "TEST RMSE:  0.7311523930107818\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 50 samples\n",
      "3.0    0.36\n",
      "2.0    0.30\n",
      "1.0    0.20\n",
      "4.0    0.12\n",
      "5.0    0.02\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.6337626098077862\n",
      "Val RMSE: 1.0488088481701516\n",
      "TEST RMSE:  0.7370080936128186\n",
      "\n",
      "Mean Train RMSE: 0.6309185704412663 std: 0.002493320731969989\n",
      "Mean Val RMSE: 1.0198513709459782 std: 0.10809187476072589\n",
      "Mean Test RMSE: 0.7326430633147186 std: 0.0025136444302648287\n",
      "Exact matches:  0.6311059007424775\n",
      "Missed by 1:  0.32317311449785074\n",
      "Missed by 2:  0.041422430636967565\n",
      "Missed by 3:  0.003907776475185619\n",
      "Missed by 4:  0.00039077764751856197\n",
      "\n",
      "Severity 1 : accuracy: 29.23 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.215 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 3.908 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.757 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.77      0.67      0.72      1124\n",
      "         2.0       0.33      0.58      0.42       492\n",
      "         3.0       0.55      0.25      0.34       405\n",
      "         4.0       0.91      0.91      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.63      2559\n",
      "   macro avg       0.51      0.48      0.48      2559\n",
      "weighted avg       0.68      0.63      0.64      2559\n",
      "\n",
      "Exact matches:  0.6314736914695538\n",
      "Missed by 1:  0.3228053237707744\n",
      "Missed by 2:  0.04316943659057996\n",
      "Missed by 3:  0.0023446658851113715\n",
      "Missed by 4:  0.00020688228398041514\n",
      "\n",
      "Severity 1 : accuracy: 29.322 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.937 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 4.124 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.75 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.014 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.77      0.67      0.71      6373\n",
      "         2.0       0.33      0.58      0.42      2747\n",
      "         3.0       0.54      0.26      0.35      2314\n",
      "         4.0       0.91      0.90      0.91      3020\n",
      "         5.0       0.25      0.04      0.07        47\n",
      "\n",
      "    accuracy                           0.63     14501\n",
      "   macro avg       0.56      0.49      0.49     14501\n",
      "weighted avg       0.68      0.63      0.64     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.69      0.80       665\n",
      "         2.0       0.33      0.57      0.42       362\n",
      "         3.0       0.60      0.63      0.61       658\n",
      "         4.0       0.55      0.19      0.28       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.42      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.68      0.76       472\n",
      "         2.0       0.48      0.74      0.59       278\n",
      "         3.0       0.51      0.47      0.49       182\n",
      "         4.0       0.40      0.15      0.21        41\n",
      "         5.0       1.00      0.09      0.17        11\n",
      "\n",
      "    accuracy                           0.63       984\n",
      "   macro avg       0.65      0.43      0.44       984\n",
      "weighted avg       0.67      0.63      0.63       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.66      0.70      5102\n",
      "         2.0       0.30      0.56      0.40      2022\n",
      "         3.0       0.50      0.06      0.11      1229\n",
      "         4.0       0.42      0.05      0.09        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.54      8457\n",
      "   macro avg       0.39      0.27      0.26      8457\n",
      "weighted avg       0.60      0.54      0.53      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.72      0.76       134\n",
      "         2.0       0.44      0.44      0.44        85\n",
      "         3.0       0.28      0.10      0.15       245\n",
      "         4.0       0.92      0.99      0.95      2716\n",
      "         5.0       0.17      0.05      0.08        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.52      0.46      0.47      3200\n",
      "weighted avg       0.85      0.89      0.86      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.95      0.72      0.82       116\n",
      "         2.0       0.39      0.63      0.48        76\n",
      "         3.0       0.57      0.61      0.59       113\n",
      "         4.0       0.43      0.10      0.17        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.60       340\n",
      "   macro avg       0.47      0.41      0.41       340\n",
      "weighted avg       0.64      0.60      0.60       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.62      0.72        84\n",
      "         2.0       0.41      0.74      0.53        43\n",
      "         3.0       0.47      0.29      0.36        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.58       159\n",
      "   macro avg       0.35      0.33      0.32       159\n",
      "weighted avg       0.64      0.58      0.58       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.74      0.66      0.70       897\n",
      "         2.0       0.30      0.55      0.39       356\n",
      "         3.0       0.63      0.09      0.15       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.54      1491\n",
      "   macro avg       0.33      0.26      0.25      1491\n",
      "weighted avg       0.61      0.54      0.54      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.59      0.54        17\n",
      "         3.0       0.33      0.09      0.15        43\n",
      "         4.0       0.92      1.00      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.90       569\n",
      "   macro avg       0.53      0.48      0.49       569\n",
      "weighted avg       0.86      0.90      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 2 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 72 samples\n",
      "1.0    0.277778\n",
      "4.0    0.263889\n",
      "2.0    0.236111\n",
      "3.0    0.194444\n",
      "5.0    0.027778\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5730204973833448\n",
      "Val RMSE: 1.0474837574980447\n",
      "TEST RMSE:  0.7089005957882041\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 53 samples\n",
      "2.0    0.339623\n",
      "4.0    0.283019\n",
      "3.0    0.245283\n",
      "1.0    0.094340\n",
      "5.0    0.037736\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.574267497233361\n",
      "Val RMSE: 0.9712858623572642\n",
      "TEST RMSE:  0.7102773704310615\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 74 samples\n",
      "1.0    0.283784\n",
      "4.0    0.270270\n",
      "3.0    0.243243\n",
      "2.0    0.202703\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5754005044803829\n",
      "Val RMSE: 0.7969536593398233\n",
      "TEST RMSE:  0.7083491366648467\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 91 samples\n",
      "3.0    0.340659\n",
      "4.0    0.230769\n",
      "2.0    0.219780\n",
      "1.0    0.186813\n",
      "5.0    0.021978\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.580636647618903\n",
      "Val RMSE: 1.0054794930723405\n",
      "TEST RMSE:  0.7135707916644894\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 77 samples\n",
      "3.0    0.311688\n",
      "1.0    0.233766\n",
      "2.0    0.233766\n",
      "4.0    0.207792\n",
      "5.0    0.012987\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5788524074150058\n",
      "Val RMSE: 0.9466276438515284\n",
      "TEST RMSE:  0.7217385982073032\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 62 samples\n",
      "3.0    0.338710\n",
      "1.0    0.258065\n",
      "2.0    0.209677\n",
      "4.0    0.193548\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5769962565746347\n",
      "Val RMSE: 0.9332565252573828\n",
      "TEST RMSE:  0.7252494147257097\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 65 samples\n",
      "2.0    0.292308\n",
      "3.0    0.276923\n",
      "4.0    0.230769\n",
      "1.0    0.184615\n",
      "5.0    0.015385\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5809005134119455\n",
      "Val RMSE: 0.9766504768063925\n",
      "TEST RMSE:  0.7094516262606688\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 68 samples\n",
      "3.0    0.352941\n",
      "2.0    0.235294\n",
      "4.0    0.205882\n",
      "1.0    0.191176\n",
      "5.0    0.014706\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.578057644009416\n",
      "Val RMSE: 0.9776923610938036\n",
      "TEST RMSE:  0.7187545130429018\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 92 samples\n",
      "3.0    0.336957\n",
      "2.0    0.304348\n",
      "1.0    0.217391\n",
      "4.0    0.141304\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.571123368257664\n",
      "Val RMSE: 0.920774721067277\n",
      "TEST RMSE:  0.705585376080491\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 65 samples\n",
      "3.0    0.353846\n",
      "2.0    0.246154\n",
      "4.0    0.184615\n",
      "1.0    0.184615\n",
      "5.0    0.030769\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5791832369598826\n",
      "Val RMSE: 0.9922778767136676\n",
      "TEST RMSE:  0.7146652260371708\n",
      "\n",
      "Mean Train RMSE: 0.5768438573344541 std: 0.003129028062782687\n",
      "Mean Val RMSE: 0.9568482377057526 std: 0.06355998045471524\n",
      "Mean Test RMSE: 0.7136542648902847 std: 0.006097210805031475\n",
      "Exact matches:  0.636967565455256\n",
      "Missed by 1:  0.32395466979288784\n",
      "Missed by 2:  0.0359515435717077\n",
      "Missed by 3:  0.0031262211801484957\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 28.839 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.606 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.65 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.601 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.66      0.72      1124\n",
      "         2.0       0.33      0.60      0.43       492\n",
      "         3.0       0.53      0.29      0.38       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.52      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6403696296807116\n",
      "Missed by 1:  0.3197020895110682\n",
      "Missed by 2:  0.037721536445762364\n",
      "Missed by 3:  0.0022067443624577614\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.315 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 11.165 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 4.938 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.613 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.67      0.72      6373\n",
      "         2.0       0.34      0.59      0.43      2747\n",
      "         3.0       0.55      0.31      0.40      2314\n",
      "         4.0       0.91      0.89      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.68      0.79       665\n",
      "         2.0       0.35      0.60      0.45       362\n",
      "         3.0       0.59      0.65      0.62       658\n",
      "         4.0       0.46      0.13      0.21       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.47      0.41      0.41      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.83      0.67      0.75       472\n",
      "         2.0       0.48      0.73      0.58       278\n",
      "         3.0       0.52      0.45      0.48       182\n",
      "         4.0       0.39      0.17      0.24        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.44      0.41      0.41       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.66      0.71      5102\n",
      "         2.0       0.31      0.57      0.41      2022\n",
      "         3.0       0.53      0.14      0.22      1229\n",
      "         4.0       0.25      0.03      0.05        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.28      0.28      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.69      0.74       134\n",
      "         2.0       0.45      0.47      0.46        85\n",
      "         3.0       0.36      0.17      0.23       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.57      0.47      0.49      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.95      0.72      0.82       116\n",
      "         2.0       0.39      0.64      0.48        76\n",
      "         3.0       0.55      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.51      0.42      0.42       340\n",
      "weighted avg       0.65      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.61      0.70        84\n",
      "         2.0       0.43      0.74      0.55        43\n",
      "         3.0       0.36      0.29      0.32        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.57       159\n",
      "   macro avg       0.33      0.33      0.31       159\n",
      "weighted avg       0.62      0.57      0.58       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.65      0.71       897\n",
      "         2.0       0.31      0.59      0.40       356\n",
      "         3.0       0.68      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.56      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.70      0.78        27\n",
      "         2.0       0.54      0.41      0.47        17\n",
      "         3.0       0.29      0.19      0.23        43\n",
      "         4.0       0.93      0.99      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.49       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 3 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 101 samples\n",
      "1.0    0.336634\n",
      "3.0    0.217822\n",
      "2.0    0.217822\n",
      "4.0    0.207921\n",
      "5.0    0.019802\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5603775410433105\n",
      "Val RMSE: 0.9698422858413324\n",
      "TEST RMSE:  0.710552405241149\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 72 samples\n",
      "2.0    0.333333\n",
      "3.0    0.250000\n",
      "4.0    0.222222\n",
      "1.0    0.166667\n",
      "5.0    0.027778\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5621308671405617\n",
      "Val RMSE: 0.9204467514322717\n",
      "TEST RMSE:  0.7290113857994567\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 92 samples\n",
      "1.0    0.336957\n",
      "3.0    0.239130\n",
      "4.0    0.228261\n",
      "2.0    0.184783\n",
      "5.0    0.010870\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5635602752979973\n",
      "Val RMSE: 0.902893898143269\n",
      "TEST RMSE:  0.7111021557336323\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 110 samples\n",
      "1.0    0.290909\n",
      "3.0    0.281818\n",
      "2.0    0.218182\n",
      "4.0    0.190909\n",
      "5.0    0.018182\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.563016166771139\n",
      "Val RMSE: 0.8994948077064754\n",
      "TEST RMSE:  0.7036442847106895\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 109 samples\n",
      "1.0    0.321101\n",
      "3.0    0.293578\n",
      "2.0    0.201835\n",
      "4.0    0.165138\n",
      "5.0    0.018349\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5653927757602892\n",
      "Val RMSE: 0.8404891542123692\n",
      "TEST RMSE:  0.7111021557336323\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 88 samples\n",
      "1.0    0.306818\n",
      "3.0    0.295455\n",
      "2.0    0.227273\n",
      "4.0    0.159091\n",
      "5.0    0.011364\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5643754545215319\n",
      "Val RMSE: 0.9107938595233581\n",
      "TEST RMSE:  0.7206549060071462\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 87 samples\n",
      "2.0    0.298851\n",
      "1.0    0.264368\n",
      "3.0    0.229885\n",
      "4.0    0.195402\n",
      "5.0    0.011494\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5670166932523126\n",
      "Val RMSE: 0.864364759104401\n",
      "TEST RMSE:  0.708624919870415\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 86 samples\n",
      "3.0    0.302326\n",
      "2.0    0.267442\n",
      "1.0    0.244186\n",
      "4.0    0.174419\n",
      "5.0    0.011628\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.563424297427687\n",
      "Val RMSE: 0.9523532664857335\n",
      "TEST RMSE:  0.7154849531956043\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 109 samples\n",
      "3.0    0.330275\n",
      "2.0    0.311927\n",
      "1.0    0.220183\n",
      "4.0    0.137615\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5611076319002093\n",
      "Val RMSE: 0.9137080416200247\n",
      "TEST RMSE:  0.705585376080491\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 82 samples\n",
      "3.0    0.341463\n",
      "2.0    0.280488\n",
      "1.0    0.207317\n",
      "4.0    0.158537\n",
      "5.0    0.012195\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5675569665461563\n",
      "Val RMSE: 0.9370425713316364\n",
      "TEST RMSE:  0.7100022290802649\n",
      "\n",
      "Mean Train RMSE: 0.5637958669661195 std: 0.002233627442107301\n",
      "Mean Val RMSE: 0.9111429395400871 std: 0.03645797437674042\n",
      "Mean Test RMSE: 0.7125764771452482 std: 0.007093459638208108\n",
      "Exact matches:  0.6389214536928488\n",
      "Missed by 1:  0.3235638921453693\n",
      "Missed by 2:  0.03477921062915201\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.074 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.45 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.767 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.601 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.66      0.73      1124\n",
      "         2.0       0.33      0.60      0.43       492\n",
      "         3.0       0.52      0.30      0.38       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.52      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6410592372939797\n",
      "Missed by 1:  0.3199779325563754\n",
      "Missed by 2:  0.03716985035514792\n",
      "Missed by 3:  0.0017929797944969312\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.591 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.785 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.069 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.654 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.67      0.73      6373\n",
      "         2.0       0.33      0.57      0.42      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.91      0.90      0.91      3020\n",
      "         5.0       0.33      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.58      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.95      0.68      0.79       665\n",
      "         2.0       0.35      0.59      0.44       362\n",
      "         3.0       0.59      0.65      0.62       658\n",
      "         4.0       0.51      0.15      0.23       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.41      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.67      0.75       472\n",
      "         2.0       0.48      0.75      0.59       278\n",
      "         3.0       0.55      0.50      0.53       182\n",
      "         4.0       0.38      0.12      0.19        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.63       984\n",
      "   macro avg       0.45      0.41      0.41       984\n",
      "weighted avg       0.66      0.63      0.63       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.67      0.72      5102\n",
      "         2.0       0.31      0.55      0.39      2022\n",
      "         3.0       0.49      0.15      0.23      1229\n",
      "         4.0       0.27      0.04      0.07        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.28      0.28      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.69      0.74       134\n",
      "         2.0       0.41      0.44      0.42        85\n",
      "         3.0       0.32      0.14      0.20       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.50      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.59      0.46      0.48      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.81       116\n",
      "         2.0       0.37      0.59      0.46        76\n",
      "         3.0       0.54      0.60      0.57       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.51      0.41      0.41       340\n",
      "weighted avg       0.64      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.55      0.66        84\n",
      "         2.0       0.41      0.74      0.53        43\n",
      "         3.0       0.30      0.25      0.27        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.53       159\n",
      "   macro avg       0.31      0.31      0.29       159\n",
      "weighted avg       0.61      0.53      0.54       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.66      0.72       897\n",
      "         2.0       0.31      0.59      0.41       356\n",
      "         3.0       0.66      0.18      0.28       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.57      1491\n",
      "   macro avg       0.35      0.29      0.28      1491\n",
      "weighted avg       0.65      0.57      0.57      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.29      0.19      0.23        43\n",
      "         4.0       0.93      0.99      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.49       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 4 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 559 samples\n",
      "1.0    0.550984\n",
      "2.0    0.252236\n",
      "3.0    0.148479\n",
      "4.0    0.044723\n",
      "5.0    0.003578\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5625611929252147\n",
      "Val RMSE: 0.7935337960728824\n",
      "TEST RMSE:  0.7143917746301688\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 537 samples\n",
      "1.0    0.567970\n",
      "2.0    0.238361\n",
      "3.0    0.150838\n",
      "4.0    0.037244\n",
      "5.0    0.005587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5615171497495659\n",
      "Val RMSE: 0.7585637708284043\n",
      "TEST RMSE:  0.7241709763377085\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 562 samples\n",
      "1.0    0.562278\n",
      "2.0    0.241993\n",
      "3.0    0.153025\n",
      "4.0    0.040925\n",
      "5.0    0.001779\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5651216691631821\n",
      "Val RMSE: 0.7674424874414038\n",
      "TEST RMSE:  0.705585376080491\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 573 samples\n",
      "1.0    0.541012\n",
      "2.0    0.258290\n",
      "3.0    0.151832\n",
      "4.0    0.041885\n",
      "5.0    0.006981\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5650538721898515\n",
      "Val RMSE: 0.7703039902189881\n",
      "TEST RMSE:  0.6966676669819362\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 562 samples\n",
      "1.0    0.569395\n",
      "2.0    0.234875\n",
      "3.0    0.154804\n",
      "4.0    0.035587\n",
      "5.0    0.005338\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5673544243521565\n",
      "Val RMSE: 0.7474749903575132\n",
      "TEST RMSE:  0.7143917746301688\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 582 samples\n",
      "1.0    0.570447\n",
      "2.0    0.226804\n",
      "3.0    0.170103\n",
      "4.0    0.029210\n",
      "5.0    0.003436\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.562062709411649\n",
      "Val RMSE: 0.7919266167537394\n",
      "TEST RMSE:  0.7255187738491009\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 555 samples\n",
      "1.0    0.569369\n",
      "2.0    0.246847\n",
      "3.0    0.144144\n",
      "4.0    0.037838\n",
      "5.0    0.001802\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5675569665461563\n",
      "Val RMSE: 0.7838459124180048\n",
      "TEST RMSE:  0.7154849531956043\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 537 samples\n",
      "1.0    0.530726\n",
      "2.0    0.266294\n",
      "3.0    0.165736\n",
      "4.0    0.033520\n",
      "5.0    0.003724\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5619945434166848\n",
      "Val RMSE: 0.7815374238505519\n",
      "TEST RMSE:  0.7138445575643235\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 582 samples\n",
      "1.0    0.549828\n",
      "2.0    0.240550\n",
      "3.0    0.171821\n",
      "4.0    0.036082\n",
      "5.0    0.001718\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5613806769927199\n",
      "Val RMSE: 0.783199886271487\n",
      "TEST RMSE:  0.7039219111901096\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 567 samples\n",
      "1.0    0.559083\n",
      "2.0    0.232804\n",
      "3.0    0.167549\n",
      "4.0    0.037037\n",
      "5.0    0.003527\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5678943763414588\n",
      "Val RMSE: 0.8240220541217403\n",
      "TEST RMSE:  0.7127488630465365\n",
      "\n",
      "Mean Train RMSE: 0.5642497581088639 std: 0.002523605617380584\n",
      "Mean Val RMSE: 0.7801850928334716 std: 0.020148181967879455\n",
      "Mean Test RMSE: 0.7126726627506148 std: 0.00833905134455194\n",
      "Exact matches:  0.6334505666275889\n",
      "Missed by 1:  0.3298163345056663\n",
      "Missed by 2:  0.03399765533411489\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 28.957 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.215 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.572 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.601 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.66      0.72      1124\n",
      "         2.0       0.32      0.58      0.42       492\n",
      "         3.0       0.52      0.29      0.37       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.63      2559\n",
      "   macro avg       0.51      0.49      0.48      2559\n",
      "weighted avg       0.69      0.63      0.64      2559\n",
      "\n",
      "Exact matches:  0.6428522170884766\n",
      "Missed by 1:  0.3180470312392249\n",
      "Missed by 2:  0.03696296807116751\n",
      "Missed by 3:  0.0021377836011309565\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.577 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.999 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.062 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.64 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.67      0.73      6373\n",
      "         2.0       0.34      0.58      0.43      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.92      0.90      0.91      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.95      0.67      0.78       665\n",
      "         2.0       0.34      0.58      0.43       362\n",
      "         3.0       0.58      0.64      0.61       658\n",
      "         4.0       0.51      0.15      0.23       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.59      1860\n",
      "   macro avg       0.48      0.41      0.41      1860\n",
      "weighted avg       0.66      0.59      0.60      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.68      0.75       472\n",
      "         2.0       0.47      0.74      0.58       278\n",
      "         3.0       0.54      0.47      0.51       182\n",
      "         4.0       0.36      0.12      0.18        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.63       984\n",
      "   macro avg       0.44      0.40      0.40       984\n",
      "weighted avg       0.65      0.63      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.77      0.67      0.72      5102\n",
      "         2.0       0.32      0.56      0.41      2022\n",
      "         3.0       0.53      0.15      0.23      1229\n",
      "         4.0       0.33      0.05      0.09        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.39      0.29      0.29      8457\n",
      "weighted avg       0.62      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.71      0.75       134\n",
      "         2.0       0.44      0.44      0.44        85\n",
      "         3.0       0.36      0.18      0.23       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.50      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.60      0.47      0.49      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.81       116\n",
      "         2.0       0.36      0.61      0.45        76\n",
      "         3.0       0.55      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.58       340\n",
      "   macro avg       0.50      0.41      0.41       340\n",
      "weighted avg       0.64      0.58      0.58       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.58      0.70        84\n",
      "         2.0       0.42      0.74      0.53        43\n",
      "         3.0       0.32      0.25      0.28        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.55       159\n",
      "   macro avg       0.32      0.32      0.30       159\n",
      "weighted avg       0.62      0.55      0.56       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.66      0.71       897\n",
      "         2.0       0.30      0.57      0.39       356\n",
      "         3.0       0.65      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.27      1491\n",
      "weighted avg       0.64      0.56      0.56      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.29      0.19      0.23        43\n",
      "         4.0       0.93      0.99      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.49       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 5 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 882 samples\n",
      "1.0    0.578231\n",
      "2.0    0.239229\n",
      "3.0    0.148526\n",
      "4.0    0.031746\n",
      "5.0    0.002268\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5616751082281154\n",
      "Val RMSE: 0.8130176704268511\n",
      "TEST RMSE:  0.7179385190091023\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 838 samples\n",
      "1.0    0.603819\n",
      "2.0    0.233890\n",
      "3.0    0.131265\n",
      "4.0    0.027446\n",
      "5.0    0.003580\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5605611427207323\n",
      "Val RMSE: 0.748108194796527\n",
      "TEST RMSE:  0.7165764640562725\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 862 samples\n",
      "1.0    0.585847\n",
      "2.0    0.236659\n",
      "3.0    0.147332\n",
      "4.0    0.029002\n",
      "5.0    0.001160\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5658668997645792\n",
      "Val RMSE: 0.7870765904057452\n",
      "TEST RMSE:  0.7122003836555365\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 872 samples\n",
      "1.0    0.574541\n",
      "2.0    0.244266\n",
      "3.0    0.147936\n",
      "4.0    0.028670\n",
      "5.0    0.004587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5666111502084755\n",
      "Val RMSE: 0.8035186381378234\n",
      "TEST RMSE:  0.710552405241149\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 864 samples\n",
      "1.0    0.590278\n",
      "2.0    0.241898\n",
      "3.0    0.134259\n",
      "4.0    0.028935\n",
      "5.0    0.004630\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.566340626846095\n",
      "Val RMSE: 0.7607257743127307\n",
      "TEST RMSE:  0.7089005957882041\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 877 samples\n",
      "1.0    0.600912\n",
      "2.0    0.231471\n",
      "3.0    0.145952\n",
      "4.0    0.019384\n",
      "5.0    0.002281\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5623352907610437\n",
      "Val RMSE: 0.7919204318323134\n",
      "TEST RMSE:  0.7217385982073032\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 876 samples\n",
      "1.0    0.598174\n",
      "2.0    0.243151\n",
      "3.0    0.127854\n",
      "4.0    0.028539\n",
      "5.0    0.002283\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5675569665461563\n",
      "Val RMSE: 0.7660012995208167\n",
      "TEST RMSE:  0.7176663148214946\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 850 samples\n",
      "1.0    0.581176\n",
      "2.0    0.254118\n",
      "3.0    0.138824\n",
      "4.0    0.023529\n",
      "5.0    0.002353\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5624715318995083\n",
      "Val RMSE: 0.7768715542254755\n",
      "TEST RMSE:  0.7089005957882041\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 874 samples\n",
      "1.0    0.575515\n",
      "2.0    0.250572\n",
      "3.0    0.141876\n",
      "4.0    0.029748\n",
      "5.0    0.002288\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5600141202506289\n",
      "Val RMSE: 0.7735620009646185\n",
      "TEST RMSE:  0.7028107472595039\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 856 samples\n",
      "1.0    0.596963\n",
      "2.0    0.235981\n",
      "3.0    0.139019\n",
      "4.0    0.025701\n",
      "5.0    0.002336\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5697129361952602\n",
      "Val RMSE: 0.769603612014762\n",
      "TEST RMSE:  0.7083491366648467\n",
      "\n",
      "Mean Train RMSE: 0.5643145773420594 std: 0.0031354040603113315\n",
      "Mean Val RMSE: 0.7790405766637664 std: 0.018894785824405947\n",
      "Mean Test RMSE: 0.7125633760491616 std: 0.005469030905049362\n",
      "Exact matches:  0.6361860101602188\n",
      "Missed by 1:  0.32708089097303633\n",
      "Missed by 2:  0.03399765533411489\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.23 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.215 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.572 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.601 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.67      0.73      1124\n",
      "         2.0       0.33      0.58      0.42       492\n",
      "         3.0       0.51      0.29      0.37       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.51      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6405765119646921\n",
      "Missed by 1:  0.32052961864698987\n",
      "Missed by 2:  0.03661816426453348\n",
      "Missed by 3:  0.0022757051237845666\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.481 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.916 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.0 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.654 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.67      0.72      6373\n",
      "         2.0       0.33      0.58      0.42      2747\n",
      "         3.0       0.55      0.31      0.40      2314\n",
      "         4.0       0.91      0.90      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.67      0.78       665\n",
      "         2.0       0.35      0.60      0.44       362\n",
      "         3.0       0.60      0.64      0.62       658\n",
      "         4.0       0.50      0.15      0.23       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.41      1860\n",
      "weighted avg       0.66      0.60      0.60      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.67      0.74       472\n",
      "         2.0       0.47      0.74      0.58       278\n",
      "         3.0       0.54      0.46      0.50       182\n",
      "         4.0       0.38      0.15      0.21        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.45      0.40      0.41       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.67      0.71      5102\n",
      "         2.0       0.31      0.55      0.40      2022\n",
      "         3.0       0.51      0.15      0.23      1229\n",
      "         4.0       0.19      0.03      0.05        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.35      0.28      0.28      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.72      0.76       134\n",
      "         2.0       0.46      0.46      0.46        85\n",
      "         3.0       0.35      0.16      0.22       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.57      0.47      0.50      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.82       116\n",
      "         2.0       0.37      0.61      0.46        76\n",
      "         3.0       0.54      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.50      0.41      0.41       340\n",
      "weighted avg       0.64      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.57      0.69        84\n",
      "         2.0       0.40      0.70      0.51        43\n",
      "         3.0       0.28      0.25      0.26        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.53       159\n",
      "   macro avg       0.31      0.30      0.29       159\n",
      "weighted avg       0.61      0.53      0.55       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.67      0.72       897\n",
      "         2.0       0.31      0.57      0.40       356\n",
      "         3.0       0.65      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.57      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.29      0.19      0.23        43\n",
      "         4.0       0.93      0.99      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.49       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 6 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 882 samples\n",
      "1.0    0.578231\n",
      "2.0    0.239229\n",
      "3.0    0.148526\n",
      "4.0    0.031746\n",
      "5.0    0.002268\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5622205563860045\n",
      "Val RMSE: 0.8053111678708459\n",
      "TEST RMSE:  0.7091761645433307\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 838 samples\n",
      "1.0    0.603819\n",
      "2.0    0.233890\n",
      "3.0    0.131265\n",
      "4.0    0.027446\n",
      "5.0    0.003580\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5597404085430514\n",
      "Val RMSE: 0.7457116861458021\n",
      "TEST RMSE:  0.7152118152000829\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 862 samples\n",
      "1.0    0.585847\n",
      "2.0    0.236659\n",
      "3.0    0.147332\n",
      "4.0    0.029002\n",
      "5.0    0.001160\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5619945434166848\n",
      "Val RMSE: 0.7684316748931673\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 872 samples\n",
      "1.0    0.574541\n",
      "2.0    0.244266\n",
      "3.0    0.147936\n",
      "4.0    0.028670\n",
      "5.0    0.004587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.564171770140334\n",
      "Val RMSE: 0.8020901594292321\n",
      "TEST RMSE:  0.7030887028753775\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 864 samples\n",
      "1.0    0.590278\n",
      "2.0    0.241898\n",
      "3.0    0.134259\n",
      "4.0    0.028935\n",
      "5.0    0.004630\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5651216691631821\n",
      "Val RMSE: 0.75\n",
      "TEST RMSE:  0.7122003836555365\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 877 samples\n",
      "1.0    0.600912\n",
      "2.0    0.231471\n",
      "3.0    0.145952\n",
      "4.0    0.019384\n",
      "5.0    0.002281\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5638321326583811\n",
      "Val RMSE: 0.7803165746480512\n",
      "TEST RMSE:  0.7241709763377085\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 876 samples\n",
      "1.0    0.598174\n",
      "2.0    0.243151\n",
      "3.0    0.127854\n",
      "4.0    0.028539\n",
      "5.0    0.002283\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5660022908180214\n",
      "Val RMSE: 0.7630149283086917\n",
      "TEST RMSE:  0.7091761645433307\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 850 samples\n",
      "1.0    0.581176\n",
      "2.0    0.254118\n",
      "3.0    0.138824\n",
      "4.0    0.023529\n",
      "5.0    0.002353\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5643754545215319\n",
      "Val RMSE: 0.7730763602798428\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 874 samples\n",
      "1.0    0.575515\n",
      "2.0    0.250572\n",
      "3.0    0.141876\n",
      "4.0    0.029748\n",
      "5.0    0.002288\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5615853736912245\n",
      "Val RMSE: 0.7787215950867564\n",
      "TEST RMSE:  0.6977886132691821\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 856 samples\n",
      "1.0    0.596963\n",
      "2.0    0.235981\n",
      "3.0    0.139019\n",
      "4.0    0.025701\n",
      "5.0    0.002336\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5692420155794661\n",
      "Val RMSE: 0.7688442594746369\n",
      "TEST RMSE:  0.7119259855010034\n",
      "\n",
      "Mean Train RMSE: 0.5638286214917881 std: 0.0025261392961094947\n",
      "Mean Val RMSE: 0.7735518406137026 std: 0.01839829316338363\n",
      "Mean Test RMSE: 0.7105492549250851 std: 0.006599597271065811\n",
      "Exact matches:  0.6361860101602188\n",
      "Missed by 1:  0.32590855803048063\n",
      "Missed by 2:  0.035169988276670575\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.035 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.254 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.728 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.601 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.66      0.72      1124\n",
      "         2.0       0.33      0.59      0.42       492\n",
      "         3.0       0.52      0.30      0.38       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.51      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6429901386111303\n",
      "Missed by 1:  0.3178401489552445\n",
      "Missed by 2:  0.03723881111647473\n",
      "Missed by 3:  0.0019309013171505413\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.674 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.841 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.131 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.647 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.68      0.73      6373\n",
      "         2.0       0.33      0.57      0.42      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.91      0.90      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.68      0.79       665\n",
      "         2.0       0.34      0.58      0.43       362\n",
      "         3.0       0.58      0.64      0.61       658\n",
      "         4.0       0.49      0.15      0.23       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.59      1860\n",
      "   macro avg       0.47      0.41      0.41      1860\n",
      "weighted avg       0.65      0.59      0.60      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.67      0.74       472\n",
      "         2.0       0.47      0.74      0.57       278\n",
      "         3.0       0.54      0.47      0.50       182\n",
      "         4.0       0.38      0.15      0.21        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.45      0.40      0.41       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.67      0.72      5102\n",
      "         2.0       0.31      0.55      0.40      2022\n",
      "         3.0       0.53      0.16      0.25      1229\n",
      "         4.0       0.25      0.04      0.07        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.29      0.29      8457\n",
      "weighted avg       0.62      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.72      0.76       134\n",
      "         2.0       0.45      0.45      0.45        85\n",
      "         3.0       0.35      0.17      0.23       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.57      0.47      0.50      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.82       116\n",
      "         2.0       0.39      0.62      0.47        76\n",
      "         3.0       0.55      0.60      0.58       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.60       340\n",
      "   macro avg       0.51      0.42      0.42       340\n",
      "weighted avg       0.65      0.60      0.60       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.57      0.69        84\n",
      "         2.0       0.41      0.72      0.52        43\n",
      "         3.0       0.29      0.25      0.27        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.54       159\n",
      "   macro avg       0.31      0.31      0.30       159\n",
      "weighted avg       0.61      0.54      0.55       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.66      0.71       897\n",
      "         2.0       0.30      0.57      0.40       356\n",
      "         3.0       0.64      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.56      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.70      0.78        27\n",
      "         2.0       0.54      0.41      0.47        17\n",
      "         3.0       0.29      0.19      0.23        43\n",
      "         4.0       0.93      0.99      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.49       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 7 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 882 samples\n",
      "1.0    0.578231\n",
      "2.0    0.239229\n",
      "3.0    0.148526\n",
      "4.0    0.031746\n",
      "5.0    0.002268\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5620842439688268\n",
      "Val RMSE: 0.8088232262601442\n",
      "TEST RMSE:  0.7171215964774924\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 838 samples\n",
      "1.0    0.603819\n",
      "2.0    0.233890\n",
      "3.0    0.131265\n",
      "4.0    0.027446\n",
      "5.0    0.003580\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5596035024853492\n",
      "Val RMSE: 0.7528783272590098\n",
      "TEST RMSE:  0.7165764640562725\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 862 samples\n",
      "1.0    0.585847\n",
      "2.0    0.236659\n",
      "3.0    0.147332\n",
      "4.0    0.029002\n",
      "5.0    0.001160\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5619945434166848\n",
      "Val RMSE: 0.7684316748931673\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 872 samples\n",
      "1.0    0.574541\n",
      "2.0    0.244266\n",
      "3.0    0.147936\n",
      "4.0    0.028670\n",
      "5.0    0.004587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5636282519311735\n",
      "Val RMSE: 0.8020901594292321\n",
      "TEST RMSE:  0.7097269810648503\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 864 samples\n",
      "1.0    0.590278\n",
      "2.0    0.241898\n",
      "3.0    0.134259\n",
      "4.0    0.028935\n",
      "5.0    0.004630\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5651216691631821\n",
      "Val RMSE: 0.75\n",
      "TEST RMSE:  0.7122003836555365\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 877 samples\n",
      "1.0    0.600912\n",
      "2.0    0.231471\n",
      "3.0    0.145952\n",
      "4.0    0.019384\n",
      "5.0    0.002281\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.563424297427687\n",
      "Val RMSE: 0.7825054052439714\n",
      "TEST RMSE:  0.7255187738491009\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 876 samples\n",
      "1.0    0.598174\n",
      "2.0    0.243151\n",
      "3.0    0.127854\n",
      "4.0    0.028539\n",
      "5.0    0.002283\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5647826029124846\n",
      "Val RMSE: 0.7532274544740891\n",
      "TEST RMSE:  0.7127488630465365\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 850 samples\n",
      "1.0    0.581176\n",
      "2.0    0.254118\n",
      "3.0    0.138824\n",
      "4.0    0.023529\n",
      "5.0    0.002353\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5643754545215319\n",
      "Val RMSE: 0.7730763602798428\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 874 samples\n",
      "1.0    0.575515\n",
      "2.0    0.250572\n",
      "3.0    0.141876\n",
      "4.0    0.029748\n",
      "5.0    0.002288\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5615853736912245\n",
      "Val RMSE: 0.7787215950867564\n",
      "TEST RMSE:  0.6977886132691821\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 856 samples\n",
      "1.0    0.596963\n",
      "2.0    0.235981\n",
      "3.0    0.139019\n",
      "4.0    0.025701\n",
      "5.0    0.002336\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5693093138022678\n",
      "Val RMSE: 0.7635076458252693\n",
      "TEST RMSE:  0.7039219111901096\n",
      "\n",
      "Mean Train RMSE: 0.5635909253320411 std: 0.0024869295386810123\n",
      "Mean Val RMSE: 0.7733261848751483 std: 0.019202556632352624\n",
      "Mean Test RMSE: 0.7118357329934379 std: 0.0070952474069597374\n",
      "Exact matches:  0.6361860101602188\n",
      "Missed by 1:  0.32590855803048063\n",
      "Missed by 2:  0.035169988276670575\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.074 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.333 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.65 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.562 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.66      0.73      1124\n",
      "         2.0       0.33      0.59      0.42       492\n",
      "         3.0       0.51      0.29      0.37       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.51      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6429211778498035\n",
      "Missed by 1:  0.31894352113647334\n",
      "Missed by 2:  0.03579063512861182\n",
      "Missed by 3:  0.0023446658851113715\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.77 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.813 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.076 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.626 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.68      0.73      6373\n",
      "         2.0       0.33      0.57      0.42      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.91      0.89      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.68      0.79       665\n",
      "         2.0       0.35      0.59      0.44       362\n",
      "         3.0       0.59      0.64      0.61       658\n",
      "         4.0       0.52      0.15      0.24       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.42      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.67      0.74       472\n",
      "         2.0       0.46      0.73      0.57       278\n",
      "         3.0       0.54      0.46      0.50       182\n",
      "         4.0       0.36      0.12      0.18        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.44      0.40      0.40       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.68      0.72      5102\n",
      "         2.0       0.31      0.55      0.40      2022\n",
      "         3.0       0.52      0.16      0.24      1229\n",
      "         4.0       0.24      0.04      0.07        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.29      0.29      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.72      0.76       134\n",
      "         2.0       0.44      0.45      0.44        85\n",
      "         3.0       0.32      0.15      0.20       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.56      0.47      0.49      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.82       116\n",
      "         2.0       0.37      0.61      0.46        76\n",
      "         3.0       0.54      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.51      0.41      0.41       340\n",
      "weighted avg       0.64      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.60      0.70        84\n",
      "         2.0       0.42      0.72      0.53        43\n",
      "         3.0       0.32      0.29      0.30        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56       159\n",
      "   macro avg       0.32      0.32      0.31       159\n",
      "weighted avg       0.63      0.56      0.57       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.66      0.72       897\n",
      "         2.0       0.31      0.58      0.40       356\n",
      "         3.0       0.64      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.57      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.28      0.19      0.22        43\n",
      "         4.0       0.93      0.98      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.48       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 8 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 882 samples\n",
      "1.0    0.578231\n",
      "2.0    0.239229\n",
      "3.0    0.148526\n",
      "4.0    0.031746\n",
      "5.0    0.002268\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5620842439688268\n",
      "Val RMSE: 0.8088232262601442\n",
      "TEST RMSE:  0.7171215964774924\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 838 samples\n",
      "1.0    0.603819\n",
      "2.0    0.233890\n",
      "3.0    0.131265\n",
      "4.0    0.027446\n",
      "5.0    0.003580\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5596035024853492\n",
      "Val RMSE: 0.7528783272590098\n",
      "TEST RMSE:  0.7165764640562725\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 862 samples\n",
      "1.0    0.585847\n",
      "2.0    0.236659\n",
      "3.0    0.147332\n",
      "4.0    0.029002\n",
      "5.0    0.001160\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5619945434166848\n",
      "Val RMSE: 0.7684316748931673\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 872 samples\n",
      "1.0    0.574541\n",
      "2.0    0.244266\n",
      "3.0    0.147936\n",
      "4.0    0.028670\n",
      "5.0    0.004587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5636282519311735\n",
      "Val RMSE: 0.8020901594292321\n",
      "TEST RMSE:  0.7097269810648503\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 864 samples\n",
      "1.0    0.590278\n",
      "2.0    0.241898\n",
      "3.0    0.134259\n",
      "4.0    0.028935\n",
      "5.0    0.004630\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5651216691631821\n",
      "Val RMSE: 0.75\n",
      "TEST RMSE:  0.7122003836555365\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 877 samples\n",
      "1.0    0.600912\n",
      "2.0    0.231471\n",
      "3.0    0.145952\n",
      "4.0    0.019384\n",
      "5.0    0.002281\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.563424297427687\n",
      "Val RMSE: 0.7825054052439714\n",
      "TEST RMSE:  0.7255187738491009\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 876 samples\n",
      "1.0    0.598174\n",
      "2.0    0.243151\n",
      "3.0    0.127854\n",
      "4.0    0.028539\n",
      "5.0    0.002283\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5647826029124846\n",
      "Val RMSE: 0.7532274544740891\n",
      "TEST RMSE:  0.7127488630465365\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 850 samples\n",
      "1.0    0.581176\n",
      "2.0    0.254118\n",
      "3.0    0.138824\n",
      "4.0    0.023529\n",
      "5.0    0.002353\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5643754545215319\n",
      "Val RMSE: 0.7730763602798428\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 874 samples\n",
      "1.0    0.575515\n",
      "2.0    0.250572\n",
      "3.0    0.141876\n",
      "4.0    0.029748\n",
      "5.0    0.002288\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5615853736912245\n",
      "Val RMSE: 0.7787215950867564\n",
      "TEST RMSE:  0.6977886132691821\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 856 samples\n",
      "1.0    0.596963\n",
      "2.0    0.235981\n",
      "3.0    0.139019\n",
      "4.0    0.025701\n",
      "5.0    0.002336\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5693093138022678\n",
      "Val RMSE: 0.7635076458252693\n",
      "TEST RMSE:  0.7039219111901096\n",
      "\n",
      "Mean Train RMSE: 0.5635909253320411 std: 0.0024869295386810123\n",
      "Mean Val RMSE: 0.7733261848751483 std: 0.019202556632352624\n",
      "Mean Test RMSE: 0.7118357329934379 std: 0.0070952474069597374\n",
      "Exact matches:  0.6361860101602188\n",
      "Missed by 1:  0.32590855803048063\n",
      "Missed by 2:  0.035169988276670575\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.074 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.333 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.65 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.562 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.66      0.73      1124\n",
      "         2.0       0.33      0.59      0.42       492\n",
      "         3.0       0.51      0.29      0.37       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.51      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6429211778498035\n",
      "Missed by 1:  0.31894352113647334\n",
      "Missed by 2:  0.03579063512861182\n",
      "Missed by 3:  0.0023446658851113715\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.77 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.813 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.076 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.626 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.68      0.73      6373\n",
      "         2.0       0.33      0.57      0.42      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.91      0.89      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.68      0.79       665\n",
      "         2.0       0.35      0.59      0.44       362\n",
      "         3.0       0.59      0.64      0.61       658\n",
      "         4.0       0.52      0.15      0.24       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.42      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.67      0.74       472\n",
      "         2.0       0.46      0.73      0.57       278\n",
      "         3.0       0.54      0.46      0.50       182\n",
      "         4.0       0.36      0.12      0.18        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.44      0.40      0.40       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.68      0.72      5102\n",
      "         2.0       0.31      0.55      0.40      2022\n",
      "         3.0       0.52      0.16      0.24      1229\n",
      "         4.0       0.24      0.04      0.07        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.29      0.29      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.72      0.76       134\n",
      "         2.0       0.44      0.45      0.44        85\n",
      "         3.0       0.32      0.15      0.20       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.56      0.47      0.49      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.82       116\n",
      "         2.0       0.37      0.61      0.46        76\n",
      "         3.0       0.54      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.51      0.41      0.41       340\n",
      "weighted avg       0.64      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.60      0.70        84\n",
      "         2.0       0.42      0.72      0.53        43\n",
      "         3.0       0.32      0.29      0.30        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56       159\n",
      "   macro avg       0.32      0.32      0.31       159\n",
      "weighted avg       0.63      0.56      0.57       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.66      0.72       897\n",
      "         2.0       0.31      0.58      0.40       356\n",
      "         3.0       0.64      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.57      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.28      0.19      0.22        43\n",
      "         4.0       0.93      0.98      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.48       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 9 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 882 samples\n",
      "1.0    0.578231\n",
      "2.0    0.239229\n",
      "3.0    0.148526\n",
      "4.0    0.031746\n",
      "5.0    0.002268\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5620842439688268\n",
      "Val RMSE: 0.8088232262601442\n",
      "TEST RMSE:  0.7171215964774924\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 838 samples\n",
      "1.0    0.603819\n",
      "2.0    0.233890\n",
      "3.0    0.131265\n",
      "4.0    0.027446\n",
      "5.0    0.003580\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5596035024853492\n",
      "Val RMSE: 0.7528783272590098\n",
      "TEST RMSE:  0.7165764640562725\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 862 samples\n",
      "1.0    0.585847\n",
      "2.0    0.236659\n",
      "3.0    0.147332\n",
      "4.0    0.029002\n",
      "5.0    0.001160\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5619945434166848\n",
      "Val RMSE: 0.7684316748931673\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 872 samples\n",
      "1.0    0.574541\n",
      "2.0    0.244266\n",
      "3.0    0.147936\n",
      "4.0    0.028670\n",
      "5.0    0.004587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5636282519311735\n",
      "Val RMSE: 0.8020901594292321\n",
      "TEST RMSE:  0.7097269810648503\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 864 samples\n",
      "1.0    0.590278\n",
      "2.0    0.241898\n",
      "3.0    0.134259\n",
      "4.0    0.028935\n",
      "5.0    0.004630\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5651216691631821\n",
      "Val RMSE: 0.75\n",
      "TEST RMSE:  0.7122003836555365\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 877 samples\n",
      "1.0    0.600912\n",
      "2.0    0.231471\n",
      "3.0    0.145952\n",
      "4.0    0.019384\n",
      "5.0    0.002281\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.563424297427687\n",
      "Val RMSE: 0.7825054052439714\n",
      "TEST RMSE:  0.7255187738491009\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 876 samples\n",
      "1.0    0.598174\n",
      "2.0    0.243151\n",
      "3.0    0.127854\n",
      "4.0    0.028539\n",
      "5.0    0.002283\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5647826029124846\n",
      "Val RMSE: 0.7532274544740891\n",
      "TEST RMSE:  0.7127488630465365\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 850 samples\n",
      "1.0    0.581176\n",
      "2.0    0.254118\n",
      "3.0    0.138824\n",
      "4.0    0.023529\n",
      "5.0    0.002353\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5643754545215319\n",
      "Val RMSE: 0.7730763602798428\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 874 samples\n",
      "1.0    0.575515\n",
      "2.0    0.250572\n",
      "3.0    0.141876\n",
      "4.0    0.029748\n",
      "5.0    0.002288\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5615853736912245\n",
      "Val RMSE: 0.7787215950867564\n",
      "TEST RMSE:  0.6977886132691821\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 856 samples\n",
      "1.0    0.596963\n",
      "2.0    0.235981\n",
      "3.0    0.139019\n",
      "4.0    0.025701\n",
      "5.0    0.002336\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5693093138022678\n",
      "Val RMSE: 0.7635076458252693\n",
      "TEST RMSE:  0.7039219111901096\n",
      "\n",
      "Mean Train RMSE: 0.5635909253320411 std: 0.0024869295386810123\n",
      "Mean Val RMSE: 0.7733261848751483 std: 0.019202556632352624\n",
      "Mean Test RMSE: 0.7118357329934379 std: 0.0070952474069597374\n",
      "Exact matches:  0.6361860101602188\n",
      "Missed by 1:  0.32590855803048063\n",
      "Missed by 2:  0.035169988276670575\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.074 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.333 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.65 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.562 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.66      0.73      1124\n",
      "         2.0       0.33      0.59      0.42       492\n",
      "         3.0       0.51      0.29      0.37       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.51      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6429211778498035\n",
      "Missed by 1:  0.31894352113647334\n",
      "Missed by 2:  0.03579063512861182\n",
      "Missed by 3:  0.0023446658851113715\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.77 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.813 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.076 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.626 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.68      0.73      6373\n",
      "         2.0       0.33      0.57      0.42      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.91      0.89      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.68      0.79       665\n",
      "         2.0       0.35      0.59      0.44       362\n",
      "         3.0       0.59      0.64      0.61       658\n",
      "         4.0       0.52      0.15      0.24       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.42      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.67      0.74       472\n",
      "         2.0       0.46      0.73      0.57       278\n",
      "         3.0       0.54      0.46      0.50       182\n",
      "         4.0       0.36      0.12      0.18        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.44      0.40      0.40       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.68      0.72      5102\n",
      "         2.0       0.31      0.55      0.40      2022\n",
      "         3.0       0.52      0.16      0.24      1229\n",
      "         4.0       0.24      0.04      0.07        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.29      0.29      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.72      0.76       134\n",
      "         2.0       0.44      0.45      0.44        85\n",
      "         3.0       0.32      0.15      0.20       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.56      0.47      0.49      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.82       116\n",
      "         2.0       0.37      0.61      0.46        76\n",
      "         3.0       0.54      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.51      0.41      0.41       340\n",
      "weighted avg       0.64      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.60      0.70        84\n",
      "         2.0       0.42      0.72      0.53        43\n",
      "         3.0       0.32      0.29      0.30        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56       159\n",
      "   macro avg       0.32      0.32      0.31       159\n",
      "weighted avg       0.63      0.56      0.57       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.66      0.72       897\n",
      "         2.0       0.31      0.58      0.40       356\n",
      "         3.0       0.64      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.57      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.28      0.19      0.22        43\n",
      "         4.0       0.93      0.98      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.48       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 10 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 882 samples\n",
      "1.0    0.578231\n",
      "2.0    0.239229\n",
      "3.0    0.148526\n",
      "4.0    0.031746\n",
      "5.0    0.002268\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5620842439688268\n",
      "Val RMSE: 0.8088232262601442\n",
      "TEST RMSE:  0.7171215964774924\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 838 samples\n",
      "1.0    0.603819\n",
      "2.0    0.233890\n",
      "3.0    0.131265\n",
      "4.0    0.027446\n",
      "5.0    0.003580\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5596035024853492\n",
      "Val RMSE: 0.7528783272590098\n",
      "TEST RMSE:  0.7165764640562725\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 862 samples\n",
      "1.0    0.585847\n",
      "2.0    0.236659\n",
      "3.0    0.147332\n",
      "4.0    0.029002\n",
      "5.0    0.001160\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5619945434166848\n",
      "Val RMSE: 0.7684316748931673\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 872 samples\n",
      "1.0    0.574541\n",
      "2.0    0.244266\n",
      "3.0    0.147936\n",
      "4.0    0.028670\n",
      "5.0    0.004587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5636282519311735\n",
      "Val RMSE: 0.8020901594292321\n",
      "TEST RMSE:  0.7097269810648503\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 864 samples\n",
      "1.0    0.590278\n",
      "2.0    0.241898\n",
      "3.0    0.134259\n",
      "4.0    0.028935\n",
      "5.0    0.004630\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5651216691631821\n",
      "Val RMSE: 0.75\n",
      "TEST RMSE:  0.7122003836555365\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 877 samples\n",
      "1.0    0.600912\n",
      "2.0    0.231471\n",
      "3.0    0.145952\n",
      "4.0    0.019384\n",
      "5.0    0.002281\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.563424297427687\n",
      "Val RMSE: 0.7825054052439714\n",
      "TEST RMSE:  0.7255187738491009\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 876 samples\n",
      "1.0    0.598174\n",
      "2.0    0.243151\n",
      "3.0    0.127854\n",
      "4.0    0.028539\n",
      "5.0    0.002283\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5647826029124846\n",
      "Val RMSE: 0.7532274544740891\n",
      "TEST RMSE:  0.7127488630465365\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 850 samples\n",
      "1.0    0.581176\n",
      "2.0    0.254118\n",
      "3.0    0.138824\n",
      "4.0    0.023529\n",
      "5.0    0.002353\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5643754545215319\n",
      "Val RMSE: 0.7730763602798428\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 874 samples\n",
      "1.0    0.575515\n",
      "2.0    0.250572\n",
      "3.0    0.141876\n",
      "4.0    0.029748\n",
      "5.0    0.002288\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5615853736912245\n",
      "Val RMSE: 0.7787215950867564\n",
      "TEST RMSE:  0.6977886132691821\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 856 samples\n",
      "1.0    0.596963\n",
      "2.0    0.235981\n",
      "3.0    0.139019\n",
      "4.0    0.025701\n",
      "5.0    0.002336\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5693093138022678\n",
      "Val RMSE: 0.7635076458252693\n",
      "TEST RMSE:  0.7039219111901096\n",
      "\n",
      "Mean Train RMSE: 0.5635909253320411 std: 0.0024869295386810123\n",
      "Mean Val RMSE: 0.7733261848751483 std: 0.019202556632352624\n",
      "Mean Test RMSE: 0.7118357329934379 std: 0.0070952474069597374\n",
      "Exact matches:  0.6361860101602188\n",
      "Missed by 1:  0.32590855803048063\n",
      "Missed by 2:  0.035169988276670575\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.074 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.333 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.65 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.562 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.66      0.73      1124\n",
      "         2.0       0.33      0.59      0.42       492\n",
      "         3.0       0.51      0.29      0.37       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.51      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6429211778498035\n",
      "Missed by 1:  0.31894352113647334\n",
      "Missed by 2:  0.03579063512861182\n",
      "Missed by 3:  0.0023446658851113715\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.77 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.813 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.076 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.626 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.68      0.73      6373\n",
      "         2.0       0.33      0.57      0.42      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.91      0.89      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.68      0.79       665\n",
      "         2.0       0.35      0.59      0.44       362\n",
      "         3.0       0.59      0.64      0.61       658\n",
      "         4.0       0.52      0.15      0.24       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.42      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.67      0.74       472\n",
      "         2.0       0.46      0.73      0.57       278\n",
      "         3.0       0.54      0.46      0.50       182\n",
      "         4.0       0.36      0.12      0.18        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.44      0.40      0.40       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.68      0.72      5102\n",
      "         2.0       0.31      0.55      0.40      2022\n",
      "         3.0       0.52      0.16      0.24      1229\n",
      "         4.0       0.24      0.04      0.07        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.29      0.29      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.72      0.76       134\n",
      "         2.0       0.44      0.45      0.44        85\n",
      "         3.0       0.32      0.15      0.20       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.56      0.47      0.49      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.82       116\n",
      "         2.0       0.37      0.61      0.46        76\n",
      "         3.0       0.54      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.51      0.41      0.41       340\n",
      "weighted avg       0.64      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.60      0.70        84\n",
      "         2.0       0.42      0.72      0.53        43\n",
      "         3.0       0.32      0.29      0.30        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56       159\n",
      "   macro avg       0.32      0.32      0.31       159\n",
      "weighted avg       0.63      0.56      0.57       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.66      0.72       897\n",
      "         2.0       0.31      0.58      0.40       356\n",
      "         3.0       0.64      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.57      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.28      0.19      0.22        43\n",
      "         4.0       0.93      0.98      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.48       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 11 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 882 samples\n",
      "1.0    0.578231\n",
      "2.0    0.239229\n",
      "3.0    0.148526\n",
      "4.0    0.031746\n",
      "5.0    0.002268\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5620842439688268\n",
      "Val RMSE: 0.8088232262601442\n",
      "TEST RMSE:  0.7171215964774924\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 838 samples\n",
      "1.0    0.603819\n",
      "2.0    0.233890\n",
      "3.0    0.131265\n",
      "4.0    0.027446\n",
      "5.0    0.003580\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5596035024853492\n",
      "Val RMSE: 0.7528783272590098\n",
      "TEST RMSE:  0.7165764640562725\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 862 samples\n",
      "1.0    0.585847\n",
      "2.0    0.236659\n",
      "3.0    0.147332\n",
      "4.0    0.029002\n",
      "5.0    0.001160\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5619945434166848\n",
      "Val RMSE: 0.7684316748931673\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 872 samples\n",
      "1.0    0.574541\n",
      "2.0    0.244266\n",
      "3.0    0.147936\n",
      "4.0    0.028670\n",
      "5.0    0.004587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5636282519311735\n",
      "Val RMSE: 0.8020901594292321\n",
      "TEST RMSE:  0.7097269810648503\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 864 samples\n",
      "1.0    0.590278\n",
      "2.0    0.241898\n",
      "3.0    0.134259\n",
      "4.0    0.028935\n",
      "5.0    0.004630\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5651216691631821\n",
      "Val RMSE: 0.75\n",
      "TEST RMSE:  0.7122003836555365\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 877 samples\n",
      "1.0    0.600912\n",
      "2.0    0.231471\n",
      "3.0    0.145952\n",
      "4.0    0.019384\n",
      "5.0    0.002281\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.563424297427687\n",
      "Val RMSE: 0.7825054052439714\n",
      "TEST RMSE:  0.7255187738491009\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 876 samples\n",
      "1.0    0.598174\n",
      "2.0    0.243151\n",
      "3.0    0.127854\n",
      "4.0    0.028539\n",
      "5.0    0.002283\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5647826029124846\n",
      "Val RMSE: 0.7532274544740891\n",
      "TEST RMSE:  0.7127488630465365\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 850 samples\n",
      "1.0    0.581176\n",
      "2.0    0.254118\n",
      "3.0    0.138824\n",
      "4.0    0.023529\n",
      "5.0    0.002353\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5643754545215319\n",
      "Val RMSE: 0.7730763602798428\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 874 samples\n",
      "1.0    0.575515\n",
      "2.0    0.250572\n",
      "3.0    0.141876\n",
      "4.0    0.029748\n",
      "5.0    0.002288\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5615853736912245\n",
      "Val RMSE: 0.7787215950867564\n",
      "TEST RMSE:  0.6977886132691821\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 856 samples\n",
      "1.0    0.596963\n",
      "2.0    0.235981\n",
      "3.0    0.139019\n",
      "4.0    0.025701\n",
      "5.0    0.002336\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5693093138022678\n",
      "Val RMSE: 0.7635076458252693\n",
      "TEST RMSE:  0.7039219111901096\n",
      "\n",
      "Mean Train RMSE: 0.5635909253320411 std: 0.0024869295386810123\n",
      "Mean Val RMSE: 0.7733261848751483 std: 0.019202556632352624\n",
      "Mean Test RMSE: 0.7118357329934379 std: 0.0070952474069597374\n",
      "Exact matches:  0.6361860101602188\n",
      "Missed by 1:  0.32590855803048063\n",
      "Missed by 2:  0.035169988276670575\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.074 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.333 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.65 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.562 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.66      0.73      1124\n",
      "         2.0       0.33      0.59      0.42       492\n",
      "         3.0       0.51      0.29      0.37       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.51      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6429211778498035\n",
      "Missed by 1:  0.31894352113647334\n",
      "Missed by 2:  0.03579063512861182\n",
      "Missed by 3:  0.0023446658851113715\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.77 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.813 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.076 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.626 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.68      0.73      6373\n",
      "         2.0       0.33      0.57      0.42      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.91      0.89      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.68      0.79       665\n",
      "         2.0       0.35      0.59      0.44       362\n",
      "         3.0       0.59      0.64      0.61       658\n",
      "         4.0       0.52      0.15      0.24       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.42      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.67      0.74       472\n",
      "         2.0       0.46      0.73      0.57       278\n",
      "         3.0       0.54      0.46      0.50       182\n",
      "         4.0       0.36      0.12      0.18        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.44      0.40      0.40       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.68      0.72      5102\n",
      "         2.0       0.31      0.55      0.40      2022\n",
      "         3.0       0.52      0.16      0.24      1229\n",
      "         4.0       0.24      0.04      0.07        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.29      0.29      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.72      0.76       134\n",
      "         2.0       0.44      0.45      0.44        85\n",
      "         3.0       0.32      0.15      0.20       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.56      0.47      0.49      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.82       116\n",
      "         2.0       0.37      0.61      0.46        76\n",
      "         3.0       0.54      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.51      0.41      0.41       340\n",
      "weighted avg       0.64      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.60      0.70        84\n",
      "         2.0       0.42      0.72      0.53        43\n",
      "         3.0       0.32      0.29      0.30        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56       159\n",
      "   macro avg       0.32      0.32      0.31       159\n",
      "weighted avg       0.63      0.56      0.57       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.66      0.72       897\n",
      "         2.0       0.31      0.58      0.40       356\n",
      "         3.0       0.64      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.57      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.28      0.19      0.22        43\n",
      "         4.0       0.93      0.98      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.48       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "rounding to 12 decimal places\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None, gpu_id=0,\n",
      "             grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=1000, n_jobs=-1,\n",
      "             num_parallel_tree=None, predictor=None, random_state=12000,\n",
      "             reg_alpha=None, reg_lambda=None, ...)\n",
      "Using cols:  ['latitude' 'longitude' 'month' 'week' 'season' 'region']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 13050 samples\n",
      "Validating on 882 samples\n",
      "1.0    0.578231\n",
      "2.0    0.239229\n",
      "3.0    0.148526\n",
      "4.0    0.031746\n",
      "5.0    0.002268\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5620842439688268\n",
      "Val RMSE: 0.8088232262601442\n",
      "TEST RMSE:  0.7171215964774924\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 838 samples\n",
      "1.0    0.603819\n",
      "2.0    0.233890\n",
      "3.0    0.131265\n",
      "4.0    0.027446\n",
      "5.0    0.003580\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5596035024853492\n",
      "Val RMSE: 0.7528783272590098\n",
      "TEST RMSE:  0.7165764640562725\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 862 samples\n",
      "1.0    0.585847\n",
      "2.0    0.236659\n",
      "3.0    0.147332\n",
      "4.0    0.029002\n",
      "5.0    0.001160\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5619945434166848\n",
      "Val RMSE: 0.7684316748931673\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 872 samples\n",
      "1.0    0.574541\n",
      "2.0    0.244266\n",
      "3.0    0.147936\n",
      "4.0    0.028670\n",
      "5.0    0.004587\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5636282519311735\n",
      "Val RMSE: 0.8020901594292321\n",
      "TEST RMSE:  0.7097269810648503\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 864 samples\n",
      "1.0    0.590278\n",
      "2.0    0.241898\n",
      "3.0    0.134259\n",
      "4.0    0.028935\n",
      "5.0    0.004630\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5651216691631821\n",
      "Val RMSE: 0.75\n",
      "TEST RMSE:  0.7122003836555365\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 877 samples\n",
      "1.0    0.600912\n",
      "2.0    0.231471\n",
      "3.0    0.145952\n",
      "4.0    0.019384\n",
      "5.0    0.002281\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.563424297427687\n",
      "Val RMSE: 0.7825054052439714\n",
      "TEST RMSE:  0.7255187738491009\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 876 samples\n",
      "1.0    0.598174\n",
      "2.0    0.243151\n",
      "3.0    0.127854\n",
      "4.0    0.028539\n",
      "5.0    0.002283\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5647826029124846\n",
      "Val RMSE: 0.7532274544740891\n",
      "TEST RMSE:  0.7127488630465365\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 850 samples\n",
      "1.0    0.581176\n",
      "2.0    0.254118\n",
      "3.0    0.138824\n",
      "4.0    0.023529\n",
      "5.0    0.002353\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5643754545215319\n",
      "Val RMSE: 0.7730763602798428\n",
      "TEST RMSE:  0.7113768716626495\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 874 samples\n",
      "1.0    0.575515\n",
      "2.0    0.250572\n",
      "3.0    0.141876\n",
      "4.0    0.029748\n",
      "5.0    0.002288\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5615853736912245\n",
      "Val RMSE: 0.7787215950867564\n",
      "TEST RMSE:  0.6977886132691821\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 13051 samples\n",
      "Validating on 856 samples\n",
      "1.0    0.596963\n",
      "2.0    0.235981\n",
      "3.0    0.139019\n",
      "4.0    0.025701\n",
      "5.0    0.002336\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  0.5693093138022678\n",
      "Val RMSE: 0.7635076458252693\n",
      "TEST RMSE:  0.7039219111901096\n",
      "\n",
      "Mean Train RMSE: 0.5635909253320411 std: 0.0024869295386810123\n",
      "Mean Val RMSE: 0.7733261848751483 std: 0.019202556632352624\n",
      "Mean Test RMSE: 0.7118357329934379 std: 0.0070952474069597374\n",
      "Exact matches:  0.6361860101602188\n",
      "Missed by 1:  0.32590855803048063\n",
      "Missed by 2:  0.035169988276670575\n",
      "Missed by 3:  0.0027354435326299334\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.074 % - prevalence: 43.923 %\n",
      "Severity 2 : accuracy: 11.333 % - prevalence: 19.226 %\n",
      "Severity 3 : accuracy: 4.65 % - prevalence: 15.826 %\n",
      "Severity 4 : accuracy: 18.562 % - prevalence: 20.594 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.43 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.66      0.73      1124\n",
      "         2.0       0.33      0.59      0.42       492\n",
      "         3.0       0.51      0.29      0.37       405\n",
      "         4.0       0.92      0.90      0.91       527\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.64      2559\n",
      "   macro avg       0.51      0.49      0.49      2559\n",
      "weighted avg       0.69      0.64      0.65      2559\n",
      "\n",
      "Exact matches:  0.6429211778498035\n",
      "Missed by 1:  0.31894352113647334\n",
      "Missed by 2:  0.03579063512861182\n",
      "Missed by 3:  0.0023446658851113715\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 29.77 % - prevalence: 43.949 %\n",
      "Severity 2 : accuracy: 10.813 % - prevalence: 18.944 %\n",
      "Severity 3 : accuracy: 5.076 % - prevalence: 15.958 %\n",
      "Severity 4 : accuracy: 18.626 % - prevalence: 20.826 %\n",
      "Severity 5 : accuracy: 0.007 % - prevalence: 0.324 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.68      0.73      6373\n",
      "         2.0       0.33      0.57      0.42      2747\n",
      "         3.0       0.54      0.32      0.40      2314\n",
      "         4.0       0.91      0.89      0.90      3020\n",
      "         5.0       0.25      0.02      0.04        47\n",
      "\n",
      "    accuracy                           0.64     14501\n",
      "   macro avg       0.57      0.50      0.50     14501\n",
      "weighted avg       0.69      0.64      0.65     14501\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.68      0.79       665\n",
      "         2.0       0.35      0.59      0.44       362\n",
      "         3.0       0.59      0.64      0.61       658\n",
      "         4.0       0.52      0.15      0.24       164\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.60      1860\n",
      "   macro avg       0.48      0.41      0.42      1860\n",
      "weighted avg       0.66      0.60      0.61      1860\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.67      0.74       472\n",
      "         2.0       0.46      0.73      0.57       278\n",
      "         3.0       0.54      0.46      0.50       182\n",
      "         4.0       0.36      0.12      0.18        41\n",
      "         5.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.62       984\n",
      "   macro avg       0.44      0.40      0.40       984\n",
      "weighted avg       0.65      0.62      0.62       984\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.76      0.68      0.72      5102\n",
      "         2.0       0.31      0.55      0.40      2022\n",
      "         3.0       0.52      0.16      0.24      1229\n",
      "         4.0       0.24      0.04      0.07        99\n",
      "         5.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.56      8457\n",
      "   macro avg       0.37      0.29      0.29      8457\n",
      "weighted avg       0.61      0.56      0.56      8457\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.72      0.76       134\n",
      "         2.0       0.44      0.45      0.44        85\n",
      "         3.0       0.32      0.15      0.20       245\n",
      "         4.0       0.93      0.98      0.95      2716\n",
      "         5.0       0.33      0.05      0.09        20\n",
      "\n",
      "    accuracy                           0.89      3200\n",
      "   macro avg       0.56      0.47      0.49      3200\n",
      "weighted avg       0.86      0.89      0.87      3200\n",
      "\n",
      "\n",
      "midwest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.94      0.72      0.82       116\n",
      "         2.0       0.37      0.61      0.46        76\n",
      "         3.0       0.54      0.58      0.56       113\n",
      "         4.0       0.67      0.14      0.23        29\n",
      "         5.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.59       340\n",
      "   macro avg       0.51      0.41      0.41       340\n",
      "weighted avg       0.64      0.59      0.59       340\n",
      "\n",
      "\n",
      "northeast\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.86      0.60      0.70        84\n",
      "         2.0       0.42      0.72      0.53        43\n",
      "         3.0       0.32      0.29      0.30        28\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56       159\n",
      "   macro avg       0.32      0.32      0.31       159\n",
      "weighted avg       0.63      0.56      0.57       159\n",
      "\n",
      "\n",
      "south\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.66      0.72       897\n",
      "         2.0       0.31      0.58      0.40       356\n",
      "         3.0       0.64      0.17      0.27       221\n",
      "         4.0       0.00      0.00      0.00        16\n",
      "         5.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.56      1491\n",
      "   macro avg       0.35      0.28      0.28      1491\n",
      "weighted avg       0.64      0.56      0.57      1491\n",
      "\n",
      "\n",
      "west\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.70      0.79        27\n",
      "         2.0       0.50      0.41      0.45        17\n",
      "         3.0       0.28      0.19      0.22        43\n",
      "         4.0       0.93      0.98      0.96       479\n",
      "         5.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.89       569\n",
      "   macro avg       0.52      0.46      0.48       569\n",
      "weighted avg       0.86      0.89      0.87       569\n",
      "\n",
      "\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0, 13):\n",
    "    print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "    print(f\"rounding to {i} decimal places\")\n",
    "    X_train.latitude = np.round(train_lats, i)\n",
    "    X_train.longitude = np.round(train_lngs, i)\n",
    "\n",
    "    X_val.latitude = np.round(val_lats, i)\n",
    "    X_val.longitude = np.round(val_lngs, i)\n",
    "\n",
    "    xgb = XGBRegressor(n_estimators=1000, max_depth=5, learning_rate=0.1, n_jobs=-1, random_state=12_000, tree_method='gpu_hist', gpu_id=0)\n",
    "    print(xgb)\n",
    "\n",
    "    te_preds, tr_preds, tr_rmse, val_rmse, test_rmse = cv_it(model=xgb, X_train=X_train, y_train=y_train, X_test=X_val, y_test=y_val, splits=10, cv_predict=True)\n",
    "\n",
    "    rounded_rmses.loc[i, 'train_rmse'] = tr_rmse\n",
    "    rounded_rmses.loc[i, 'val_rmse'] = val_rmse\n",
    "    rounded_rmses.loc[i, 'test_rmse'] = test_rmse\n",
    "\n",
    "    analyize_matches(y_val, np.clip(np.round(np.mean(te_preds, axis=1)), 1, 5))\n",
    "    analyize_matches(y_train, np.clip(np.round(tr_preds), 1, 5))\n",
    "    print('-------------------------------------------------------------------')\n",
    "    region_wise_peformance(y_data=tr_data, preds=np.clip(np.round(tr_preds), 1, 5))\n",
    "    region_wise_peformance(y_data=val_data, preds=np.clip(np.round(np.mean(te_preds, axis=1)), 1, 5))\n",
    "    print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.717254</td>\n",
       "      <td>1.048284</td>\n",
       "      <td>0.777892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.630919</td>\n",
       "      <td>1.019851</td>\n",
       "      <td>0.732643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.576844</td>\n",
       "      <td>0.956848</td>\n",
       "      <td>0.713654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.563796</td>\n",
       "      <td>0.911143</td>\n",
       "      <td>0.712576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.56425</td>\n",
       "      <td>0.780185</td>\n",
       "      <td>0.712673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.564315</td>\n",
       "      <td>0.779041</td>\n",
       "      <td>0.712563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.563829</td>\n",
       "      <td>0.773552</td>\n",
       "      <td>0.710549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.563591</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.711836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.563591</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.711836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.563591</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.711836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.563591</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.711836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.563591</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.711836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.563591</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.711836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_rmse  val_rmse test_rmse\n",
       "0    0.717254  1.048284  0.777892\n",
       "1    0.630919  1.019851  0.732643\n",
       "2    0.576844  0.956848  0.713654\n",
       "3    0.563796  0.911143  0.712576\n",
       "4     0.56425  0.780185  0.712673\n",
       "5    0.564315  0.779041  0.712563\n",
       "6    0.563829  0.773552  0.710549\n",
       "7    0.563591  0.773326  0.711836\n",
       "8    0.563591  0.773326  0.711836\n",
       "9    0.563591  0.773326  0.711836\n",
       "10   0.563591  0.773326  0.711836\n",
       "11   0.563591  0.773326  0.711836\n",
       "12   0.563591  0.773326  0.711836"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_rmses = rounded_rmses.astype(float)\n",
    "rounded_rmses.val_rmse.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------rounding to 0-----------\n",
      "0.3522273425499232\n",
      "sample  4848      41.0_-79.0\n",
      "22073    40.0_-122.0\n",
      "19116     39.0_-96.0\n",
      "6946     35.0_-119.0\n",
      "3784     35.0_-121.0\n",
      "dtype: object\n",
      "----------------rounding to 1-----------\n",
      "0.0\n",
      "sample  1489     38.9_-94.7\n",
      "310     34.4_-119.9\n",
      "1531     38.4_-94.7\n",
      "5846    36.7_-121.7\n",
      "7396     36.3_-79.1\n",
      "dtype: object\n",
      "----------------rounding to 2-----------\n",
      "0.0\n",
      "sample  21021    34.96_-120.64\n",
      "21877     35.55_-78.02\n",
      "11878    37.44_-120.04\n",
      "17452    36.77_-121.79\n",
      "5553      36.21_-119.2\n",
      "dtype: object\n",
      "----------------rounding to 3-----------\n",
      "0.0\n",
      "sample  23195     39.586_-84.995\n",
      "18262      36.708_-121.6\n",
      "14878      36.56_-121.51\n",
      "9050     35.124_-120.569\n",
      "1601      39.878_-86.018\n",
      "dtype: object\n",
      "----------------rounding to 4-----------\n",
      "0.0\n",
      "sample  13567      48.0735_-86.59\n",
      "2374     48.5588_-86.3811\n",
      "14679     39.6707_-80.865\n",
      "11677     40.397_-120.621\n",
      "10053    46.8944_-85.8513\n",
      "dtype: object\n",
      "----------------rounding to 5-----------\n",
      "0.0\n",
      "sample  8500      41.2993_-80.75833\n",
      "11855      36.9128_-121.744\n",
      "13642    42.25333_-81.10667\n",
      "18920      36.3247_-119.079\n",
      "12004         41.995_-82.45\n",
      "dtype: object\n",
      "----------------rounding to 6-----------\n",
      "0.0\n",
      "sample  20989    39.309992_-80.034025\n",
      "209              36.02_-76.31\n",
      "388          36.7025_-121.709\n",
      "2387        36.432_-79.074145\n",
      "22729       47.740833_-88.737\n",
      "dtype: object\n",
      "----------------rounding to 7-----------\n",
      "0.0\n",
      "sample  337          35.4265_-118.832\n",
      "18969         36.7254_-121.73\n",
      "3514        41.935_-81.478333\n",
      "14475      41.72966_-77.29218\n",
      "15448    41.006794_-81.003795\n",
      "dtype: object\n",
      "----------------rounding to 8-----------\n",
      "0.0\n",
      "sample  5652       36.6039_-119.467\n",
      "1975       34.9576_-120.631\n",
      "13559        32.798_-117.19\n",
      "19643    44.37343_-115.9853\n",
      "5799     41.3458_-109.45533\n",
      "dtype: object\n",
      "----------------rounding to 9-----------\n",
      "0.0\n",
      "sample  2893     35.04_-79.120118915\n",
      "6329        33.4975_-117.666\n",
      "16437       34.9454_-120.417\n",
      "19487     38.75119_-89.99254\n",
      "11536        42.416667_-79.8\n",
      "dtype: object\n",
      "----------------rounding to 10-----------\n",
      "0.0\n",
      "sample  5831     41.353881_-80.754718\n",
      "13080    39.008849_-94.684084\n",
      "4357         34.9454_-120.417\n",
      "19457    41.388596_-80.750147\n",
      "15256           42.11_-81.575\n",
      "dtype: object\n",
      "----------------rounding to 11-----------\n",
      "0.0\n",
      "sample  8387     34.9454_-120.417\n",
      "11531    36.7025_-121.709\n",
      "10382     34.413_-119.694\n",
      "15256       42.11_-81.575\n",
      "17927    36.4638_-119.223\n",
      "dtype: object\n",
      "----------------rounding to 12-----------\n",
      "0.0\n",
      "sample  7143              34.4051_-119.739\n",
      "9773              33.4975_-117.666\n",
      "19754     44.08673463_-71.00934545\n",
      "23235      34.941_-79.101086865758\n",
      "5082     35.00098_-78.859295017782\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 13):\n",
    "    print(f'----------------rounding to {i}-----------')\n",
    "    lats, lngs = np.round(test_lats, i), np.round(test_lngs, i)\n",
    "    tr_lats,tr_lngs = np.round(train_lats, i), np.round(train_lngs, i)\n",
    "\n",
    "    # % of lats, lngs in train set\n",
    "    lat_lng = lats.astype(str) + '_' + lngs.astype(str)\n",
    "    tr_lat_lng = tr_lats.astype(str) + '_' + tr_lngs.astype(str)\n",
    "    \n",
    "    print(lat_lng.isin(tr_lat_lng).sum()/len(lat_lng))\n",
    "    print('sample ', lat_lng.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no use by rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Believe in Stratified KFold??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['month','week','season']]\n",
    "X_val = X_val[['month','week','season']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.to_list() == X_val.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23563</th>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23569</th>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       month  week  season\n",
       "1          8    35       3\n",
       "12        11    44       4\n",
       "14         8    35       3\n",
       "15         8    35       3\n",
       "16         1     2       1\n",
       "...      ...   ...     ...\n",
       "23556      7    28       3\n",
       "23560      9    38       4\n",
       "23563      7    29       3\n",
       "23565     12    49       1\n",
       "23569      8    36       3\n",
       "\n",
       "[6510 rows x 3 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test[X_train.columns]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_test.columns.tolist() == X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month     0\n",
       "week      0\n",
       "season    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.isin(X_train).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17060, 3), (17060,))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_X_train = pd.concat([X_train, X_val], axis=0)\n",
    "all_y_train = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "assert all_X_train.shape[0] == all_y_train.shape[0]\n",
    "assert all_X_train.columns.tolist() == X_test.columns.tolist()\n",
    "\n",
    "all_X_train.shape, all_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.week = X_test.week.astype('int')\n",
    "# X_test.region = X_test.region.map(reg_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cols:  ['month' 'week' 'season']\n",
      "----------------------------------Fold-0------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439625\n",
      "4.0    0.208089\n",
      "2.0    0.189918\n",
      "3.0    0.159437\n",
      "5.0    0.002931\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.1790361959835518\n",
      "Val RMSE: 1.1714161127718152\n",
      "----------------------------------Fold-1------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439625\n",
      "4.0    0.208089\n",
      "2.0    0.189918\n",
      "3.0    0.159437\n",
      "5.0    0.002931\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.1788704652301056\n",
      "Val RMSE: 1.1744146230853754\n",
      "----------------------------------Fold-2------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439625\n",
      "4.0    0.208089\n",
      "2.0    0.189918\n",
      "3.0    0.158851\n",
      "5.0    0.003517\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.1783178610347256\n",
      "Val RMSE: 1.1779032377554\n",
      "----------------------------------Fold-3------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439625\n",
      "4.0    0.208089\n",
      "2.0    0.189332\n",
      "3.0    0.159437\n",
      "5.0    0.003517\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.1800576879680058\n",
      "Val RMSE: 1.1646412385524259\n",
      "----------------------------------Fold-4------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439625\n",
      "4.0    0.207503\n",
      "2.0    0.189918\n",
      "3.0    0.159437\n",
      "5.0    0.003517\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.1770735530446097\n",
      "Val RMSE: 1.1890473197994087\n",
      "----------------------------------Fold-5------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439625\n",
      "4.0    0.207503\n",
      "2.0    0.189918\n",
      "3.0    0.159437\n",
      "5.0    0.003517\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.1775714340198717\n",
      "Val RMSE: 1.1870737959166373\n",
      "----------------------------------Fold-6------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439625\n",
      "4.0    0.207503\n",
      "2.0    0.189918\n",
      "3.0    0.159437\n",
      "5.0    0.003517\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.178124388330032\n",
      "Val RMSE: 1.1811334393883133\n",
      "----------------------------------Fold-7------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439039\n",
      "4.0    0.208089\n",
      "2.0    0.189918\n",
      "3.0    0.159437\n",
      "5.0    0.003517\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.1765754613855015\n",
      "Val RMSE: 1.193721362662341\n",
      "----------------------------------Fold-8------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439039\n",
      "4.0    0.208089\n",
      "2.0    0.189918\n",
      "3.0    0.159437\n",
      "5.0    0.003517\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.1789257114032612\n",
      "Val RMSE: 1.1764093839675827\n",
      "----------------------------------Fold-9------------------------------------\n",
      "Training on 15354 samples\n",
      "Validating on 1706 samples\n",
      "1.0    0.439039\n",
      "4.0    0.208089\n",
      "2.0    0.189918\n",
      "3.0    0.159437\n",
      "5.0    0.003517\n",
      "Name: severity, dtype: float64\n",
      "Train RMSE:  1.176962884225914\n",
      "Val RMSE: 1.1922473287236877\n",
      "\n",
      "Mean Train RMSE: 1.1781515642625577 std: 0.0010467146793507519\n",
      "Mean Val RMSE: 1.1808007842622987 std: 0.009072472720988612\n",
      "Mean Test RMSE: 0.0 std: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6510, 10)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb = XGBRegressor(n_estimators=1000, max_depth=3, learning_rate=0.1, n_jobs=-1, random_state=12_000, tree_method='gpu_hist', gpu_id=0)\n",
    "test_preds_cv, train_preds_cv = cv_it(xgb, all_X_train, all_y_train, X_test=X_test, splits=10, cv_predict=True)\n",
    "test_preds_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 2., ..., 2., 2., 2.])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_rmr = np.round(np.mean(np.round(test_preds_cv), axis=1))\n",
    "preds_rmr  # mean of rounded predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 2., ..., 2., 2., 2.])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_rm = np.round(np.mean(test_preds_cv, axis=1))\n",
    "preds_rm # rounded mean s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matches:  1.0\n",
      "Missed by 1:  0.0\n",
      "Missed by 2:  0.0\n",
      "Missed by 3:  0.0\n",
      "Missed by 4:  0.0\n",
      "\n",
      "Severity 1 : accuracy: 1.874 % - prevalence: 1.874 %\n",
      "Severity 2 : accuracy: 85.253 % - prevalence: 85.253 %\n",
      "Severity 3 : accuracy: 11.644 % - prevalence: 11.644 %\n",
      "Severity 4 : accuracy: 1.214 % - prevalence: 1.214 %\n",
      "Severity 5 : accuracy: 0.0 % - prevalence: 0.0 %\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       122\n",
      "         2.0       1.00      1.00      1.00      5550\n",
      "         3.0       1.00      1.00      1.00       758\n",
      "         4.0       1.00      1.00      1.00        79\n",
      "         6.0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00      6510\n",
      "   macro avg       1.00      1.00      1.00      6510\n",
      "weighted avg       1.00      1.00      1.00      6510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyize_matches(preds_rmr, preds_rm)\n",
    "# 265.0 samples are off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.011981566820276499 * len(test_preds_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    0.852535\n",
       "3.0    0.116436\n",
       "1.0    0.018740\n",
       "4.0    0.012135\n",
       "5.0    0.000154\n",
       "dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.clip(preds_rm, 1, 5)).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    0.852535\n",
       "3.0    0.116436\n",
       "1.0    0.018740\n",
       "4.0    0.012135\n",
       "5.0    0.000154\n",
       "dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.clip(preds_rmr, 1, 5)).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    0.852814\n",
       "3.0    0.111723\n",
       "1.0    0.026026\n",
       "4.0    0.009437\n",
       "dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds_cv = np.round(train_preds_cv)\n",
    "pd.Series(np.clip(train_preds_cv, 1, 5)).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.439449\n",
       "4.0    0.207913\n",
       "2.0    0.189859\n",
       "3.0    0.159379\n",
       "5.0    0.003400\n",
       "Name: severity, dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.severity.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make submissions\n",
    "\n",
    "sub_format.severity = preds_rmr.astype('int')\n",
    "sub_format.to_csv('../submissions/to submit/xgb_1k_without_latlng_reg_cv_preds_rmr.csv', index=False)\n",
    "\n",
    "sub_format.severity = preds_rm.astype('int')\n",
    "sub_format.to_csv('../submissions/to submit/xgb_1k_without_latlng_reg_cv_preds_rm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(preds_rmr, preds_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "186db977f413ae6598820b658aed1651932768522d0652c07d949b3f5fb38b63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
